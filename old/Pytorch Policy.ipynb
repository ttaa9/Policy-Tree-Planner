{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch, torch.autograd as autograd\n",
    "import torch.nn as nn, torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable as avar\n",
    "    \n",
    "from SimpleTask import SimpleGridTask\n",
    "from TransportTask import TransportTask\n",
    "from NavTask import NavigationTask\n",
    "from SeqData import SeqData\n",
    "\n",
    "import os, sys, pickle, numpy as np, numpy.random as npr, random as r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimplePolicy(nn.Module):\n",
    "    \n",
    "    def __init__(self,obs_space,act_space, h_size=100):\n",
    "        super(SimplePolicy, self).__init__()\n",
    "        self.action_size = act_space\n",
    "        self.state_space = obs_space\n",
    "        \n",
    "        self.layer1 = nn.Linear(self.state_space,h_size)\n",
    "        self.layer2 = nn.Linear(self.h_size, self.h_size)\n",
    "        self.layer3 = nn.Linear(self.h_size, self.action_size)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        output = F.relu(self.layer1(state))\n",
    "        output = F.relu(self.layer2(output))\n",
    "        output = self.layer3(output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return output\n",
    "    \n",
    "    def train(self, train, valid, n_epochs=1500, n_episodes=500, lr=0.0003, printEvery = 10):\n",
    "        \n",
    "        optimizer = optim.Adam(self.parameters(), lr = lr)\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            if epoch % printEvery == 0: print('Epoch:',epoch, end='')\n",
    "            loss =0.0\n",
    "            self.zero_grad()\n",
    "            for episode in range(n_episodes):\n",
    "                \n",
    "                \n",
    "    def policy_rollout(env, episode_length):\n",
    "        obs, acts, rews = [], [], []\n",
    "\n",
    "        for i in range(0, episode_length): \n",
    "\n",
    "            state = env.getStateRep()\n",
    "            obs.append(state)\n",
    "            actionProb  = self.forward(state)\n",
    "\n",
    "            action = torch.max(actionProb)\n",
    "\n",
    "            env.performAction(action)\n",
    "            newState  = env.getStateRep()\n",
    "            reward = env.getReward() \n",
    "\n",
    "            #acts.append(sampleAction)\n",
    "            acts.append(action)\n",
    "            rews.append(reward)\n",
    "        #rews = discountedReward(rews)\n",
    "        return obs, acts, rews\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
