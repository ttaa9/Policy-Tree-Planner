{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, numpy.random as npr, random as r\n",
    "import tensorflow as tf  \n",
    "from NavTask import NavigationTask\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_sample(logits, d):\n",
    "    value = tf.squeeze(tf.multinomial(logits - tf.reduce_max(logits, [1], keep_dims=True), 1), [1])\n",
    "    return tf.one_hot(value, d)\n",
    "\n",
    "# compute discounted future rewards\n",
    "def discountedReward(reward, discount_factor = 0.1):\n",
    "    N = len(reward)\n",
    "    discounted_rewards = np.zeros(N)\n",
    "    r =0\n",
    "    for t in reversed(range(5)):\n",
    "        # future discounted reward from now on\n",
    "        r = reward[t] + discount_factor * r\n",
    "        discounted_rewards[t] = r\n",
    "    return discounted_rewards\n",
    "\n",
    "class SimplePolicy(object):\n",
    "    \n",
    "    def __init__(self,obs_space,act_space, h_size=100):\n",
    "       \n",
    "        print(\"Observation Space: \" , obs_space)\n",
    "        print(\"Action Space: \", act_space)\n",
    "        \n",
    "        # Input space: [Episode_length, observations], output:[Episode_Length,action_space]\n",
    "        self.input = tf.placeholder(tf.float32, [None] + list(obs_space))\n",
    "        hidden = slim.fully_connected(self.input,h_size,biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "        self.output = slim.fully_connected(hidden,act_space,activation_fn=tf.nn.softmax,biases_initializer=None)\n",
    "        self.log_prob = log_prob = tf.log(self.output)\n",
    "        \n",
    "        # sample: [Episode_length, action_space]\n",
    "        self.sample = categorical_sample(self.output, act_space)[0, :]\n",
    "        \n",
    "        self.targetAction = tf.placeholder(tf.float32, [None, act_space], name=\"action\")\n",
    "        self.reward = tf.placeholder(tf.float32, [None], name=\"reward\")\n",
    "        self.cumaltiveReward = tf.reduce_sum(self.reward)\n",
    "        \n",
    "        self.entropy =  tf.reduce_mean(tf.reduce_sum(self.output  * log_prob, 1))\n",
    "        self.crossEntropy = tf.reduce_sum(log_prob * self.targetAction, 1)\n",
    "        self.loss = -tf.reduce_mean(self.crossEntropy * self.cumaltiveReward) + 0.3*self.entropy\n",
    "        self._train = tf.train.AdamOptimizer(0.0003).minimize(self.loss)\n",
    "        \n",
    "\n",
    "        \n",
    "    def act_inference(self, ob):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run([self.output, self.sample], {self.input: [ob]})\n",
    "\n",
    "    def train_step(self, obs, acts, reward):\n",
    "        sess = tf.get_default_session()\n",
    "        batch_feed = { self.input: obs, self.targetAction: acts, self.reward: reward}\n",
    "        return sess.run([self._train, self.loss, self.cumaltiveReward, self.output,self.targetAction ], feed_dict=batch_feed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policyRollout(agent, hparams):\n",
    "    \n",
    "    #\"Runs one episode\"\n",
    "    episode_length = hparams['epiode_length']\n",
    "    env_size = hparams['env_size']\n",
    "    env = NavigationTask(env_size,env_size)\n",
    "    obs, acts, rews = [], [], []\n",
    "    \n",
    "    for i in range(0, episode_length): \n",
    "        \n",
    "        state = env.getStateRep(False)\n",
    "        obs.append(state)\n",
    "        actionProb, sampleAction  = agent.act_inference(state)\n",
    "      \n",
    "        action = actionProb.argmax()\n",
    "        sampleActionIndex = sampleAction.argmax()\n",
    "        \n",
    "        env.performAction(action)\n",
    "        newState  = env.getStateRep()\n",
    "        reward = env.getReward() \n",
    "    \n",
    "        #acts.append(sampleAction)\n",
    "        values = [action]\n",
    "        acts.append(np.squeeze(np.eye( hparams['num_actions'])[values]))\n",
    "        rews.append(reward)\n",
    "    #rews = discountedReward(rews)\n",
    "    return obs, acts, rews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # hyper parameters\n",
    "    env = NavigationTask()\n",
    "    input_size = np.shape(env.getStateRep(False))\n",
    "    hparams = {\n",
    "            'input_size': input_size,\n",
    "            'num_actions': 10,\n",
    "            'learning_rate': 0.1,\n",
    "            'epiode_length': 7,\n",
    "            'env_size': 15\n",
    "    }\n",
    "\n",
    "    # environment params\n",
    "    eparams = {\n",
    "            'num_batches': 100,\n",
    "            'ep_per_batch': 20000\n",
    "    }\n",
    "\n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "\n",
    "        pi = SimplePolicy(hparams['input_size'], hparams['num_actions'])\n",
    "\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        for batch in range(0, eparams['num_batches']):\n",
    "            print('=====\\nBATCH {}\\n===='.format(batch))\n",
    "            num = 0\n",
    "            for i in range(0, eparams['ep_per_batch']):\n",
    "                obs, acts, rews = policyRollout(pi, hparams)\n",
    "                num += 1 if 1 in rews else 0\n",
    "                pi.train_step(obs, acts, rews)\n",
    "            print(\"number of times reward\", num)\n",
    "            obs, acts, rews = policyRollout(pi, hparams)\n",
    "#             print(\"loss\",pi.train_step(obs, acts, rews))\n",
    "            print(\"Observation\", obs)\n",
    "            print(\"acts\", [np.argmax(a) for a in acts])\n",
    "            print(\"rews\", rews)\n",
    "        print(policyRollout(pi, hparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space:  (8,)\n",
      "Action Space:  10\n",
      "=====\n",
      "BATCH 0\n",
      "====\n",
      "number of times reward 0\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  5.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0., 10.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0., 14.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0., 14.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0., 14.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0., 14.,  1.,  0.,  0.,  0., 14., 14.])]\n",
      "acts [9, 9, 9, 5, 5, 5, 5]\n",
      "rews [0, 0, 0, 0, 0, 0, 0]\n",
      "=====\n",
      "BATCH 1\n",
      "====\n",
      "number of times reward 1\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  0.,  0.,  0.,  1.,  0., 14., 14.]), array([ 0.,  0.,  0.,  0.,  1.,  0., 14., 14.]), array([ 0.,  0.,  0.,  0.,  1.,  0., 14., 14.]), array([ 0.,  0.,  0.,  0.,  1.,  0., 14., 14.]), array([ 0.,  0.,  0.,  0.,  1.,  0., 14., 14.]), array([ 0.,  0.,  0.,  0.,  1.,  0., 14., 14.])]\n",
      "acts [3, 8, 8, 8, 8, 8, 8]\n",
      "rews [0, 0, 0, 0, 0, 0, 0]\n",
      "=====\n",
      "BATCH 2\n",
      "====\n",
      "number of times reward 2\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  3.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  6.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  6.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  6.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  6.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  6.,  0.,  0.,  0.,  1., 14., 14.])]\n",
      "acts [7, 7, 4, 9, 9, 9, 9]\n",
      "rews [0, 0, 0, 0, 0, 0, 0]\n",
      "=====\n",
      "BATCH 3\n",
      "====\n",
      "number of times reward 2\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  0.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  0.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  0.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  0.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  0.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  0.,  0.,  0.,  0.,  1., 14., 14.])]\n",
      "acts [4, 8, 8, 8, 8, 8, 8]\n",
      "rews [0, 0, 0, 0, 0, 0, 0]\n",
      "=====\n",
      "BATCH 4\n",
      "====\n",
      "number of times reward 2\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  0.,  0.,  0.,  1.,  0., 14., 14.]), array([ 0.,  0.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  0.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  0.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  0.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  0.,  0.,  0.,  0.,  1., 14., 14.])]\n",
      "acts [3, 4, 5, 5, 5, 5, 5]\n",
      "rews [0, 0, 0, 0, 0, 0, 0]\n",
      "=====\n",
      "BATCH 5\n",
      "====\n",
      "number of times reward 2\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  0.,  0.,  1.,  0.,  0., 14., 14.]), array([ 0.,  0.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  0.,  0.,  1.,  0.,  0., 14., 14.]), array([ 0.,  0.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  0.,  0.,  1.,  0.,  0., 14., 14.]), array([ 0.,  0.,  1.,  0.,  0.,  0., 14., 14.])]\n",
      "acts [2, 1, 2, 1, 2, 1, 2]\n",
      "rews [0, 0, 0, 0, 0, 0, 0]\n",
      "=====\n",
      "BATCH 6\n",
      "====\n",
      "number of times reward 3\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  4.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  9.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0., 14.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0., 14.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0., 14.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0., 14.,  1.,  0.,  0.,  0., 14., 14.])]\n",
      "acts [8, 9, 9, 8, 8, 8, 8]\n",
      "rews [0, 0, 0, 0, 0, 0, 0]\n",
      "=====\n",
      "BATCH 7\n",
      "====\n",
      "number of times reward 8\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  0.,  0.,  1.,  0.,  0., 14., 14.]), array([ 0.,  0.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  0.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  0.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  0.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  0.,  0.,  0.,  0.,  1., 14., 14.])]\n",
      "acts [2, 4, 6, 6, 6, 6, 6]\n",
      "rews [0, 0, 0, 0, 0, 0, 0]\n",
      "=====\n",
      "BATCH 8\n",
      "====\n",
      "number of times reward 13\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  0.,  0.,  0.,  1.,  0., 14., 14.]), array([ 0.,  0.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  0.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  0.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  0.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  0.,  0.,  0.,  0.,  1., 14., 14.])]\n",
      "acts [3, 4, 6, 6, 6, 6, 6]\n",
      "rews [0, 0, 0, 0, 0, 0, 0]\n",
      "=====\n",
      "BATCH 9\n",
      "====\n",
      "number of times reward 10\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  5.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  9.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  9.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  9.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  9.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  9.,  0.,  0.,  0.,  1., 14., 14.])]\n",
      "acts [9, 8, 4, 9, 9, 9, 9]\n",
      "rews [0, 0, 0, 0, 0, 0, 0]\n",
      "=====\n",
      "BATCH 10\n",
      "====\n",
      "number of times reward 5\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  3.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  6.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  6.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  6.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  6.,  0.,  0.,  0.,  1., 14., 14.]), array([ 0.,  6.,  0.,  0.,  0.,  1., 14., 14.])]\n",
      "acts [7, 7, 4, 9, 9, 9, 9]\n",
      "rews [0, 0, 0, 0, 0, 0, 0]\n",
      "=====\n",
      "BATCH 11\n",
      "====\n",
      "number of times reward 8\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  5.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  5.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  5.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  5.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  5.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  5.,  1.,  0.,  0.,  0., 14., 14.])]\n",
      "acts [9, 0, 0, 0, 0, 0, 0]\n",
      "rews [0, 0, 0, 0, 0, 0, 0]\n",
      "=====\n",
      "BATCH 12\n",
      "====\n",
      "number of times reward 13425\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  5.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0., 10.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0., 14.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0., 14.,  0.,  1.,  0.,  0., 14., 14.]), array([ 5., 14.,  0.,  1.,  0.,  0., 14., 14.]), array([10., 14.,  0.,  1.,  0.,  0., 14., 14.])]\n",
      "acts [9, 9, 9, 2, 9, 9, 9]\n",
      "rews [0, 0, 0, 0, 0, 0, 1]\n",
      "=====\n",
      "BATCH 13\n",
      "====\n",
      "number of times reward 20000\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  5.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0., 10.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0., 14.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0., 14.,  0.,  1.,  0.,  0., 14., 14.]), array([ 5., 14.,  0.,  1.,  0.,  0., 14., 14.]), array([10., 14.,  0.,  1.,  0.,  0., 14., 14.])]\n",
      "acts [9, 9, 9, 2, 9, 9, 9]\n",
      "rews [0, 0, 0, 0, 0, 0, 1]\n",
      "=====\n",
      "BATCH 14\n",
      "====\n",
      "number of times reward 20000\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  5.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0., 10.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0., 14.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0., 14.,  0.,  1.,  0.,  0., 14., 14.]), array([ 5., 14.,  0.,  1.,  0.,  0., 14., 14.]), array([10., 14.,  0.,  1.,  0.,  0., 14., 14.])]\n",
      "acts [9, 9, 9, 2, 9, 9, 9]\n",
      "rews [0, 0, 0, 0, 0, 0, 1]\n",
      "=====\n",
      "BATCH 15\n",
      "====\n",
      "number of times reward 20000\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0.,  5.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0., 10.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0., 14.,  1.,  0.,  0.,  0., 14., 14.]), array([ 0., 14.,  0.,  1.,  0.,  0., 14., 14.]), array([ 5., 14.,  0.,  1.,  0.,  0., 14., 14.]), array([10., 14.,  0.,  1.,  0.,  0., 14., 14.])]\n",
      "acts [9, 9, 9, 2, 9, 9, 9]\n",
      "rews [0, 0, 0, 0, 0, 0, 1]\n",
      "=====\n",
      "BATCH 16\n",
      "====\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-167e4af03d71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-4e649f00c5e3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicyRollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mnum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrews\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"number of times reward\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicyRollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-931e4e3abd04>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, obs, acts, reward)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mbatch_feed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargetAction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0macts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumaltiveReward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargetAction\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_feed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/pytorch/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1311\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1313\u001b[0;31m       \u001b[0mfeeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1314\u001b[0m       \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_name_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_name_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1311\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1313\u001b[0;31m       \u001b[0mfeeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1314\u001b[0m       \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_name_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_name_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mname\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;34m\"\"\"The string name of this tensor.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Operation was not named: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"%s:%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'python' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-371b8c7a7c2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'python' is not defined"
     ]
    }
   ],
   "source": [
    "python.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
