{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np, numpy.random as npr, random as r\n",
    "import tensorflow as tf  \n",
    "from NavTask import NavigationTask\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Node --> Children  \n",
    "\n",
    "# From: https://github.com/ericjang/gumbel-softmax\n",
    "class GumbelSoftmax(object):\n",
    "    def sample_gumbel(shape, eps=1e-20): \n",
    "      \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "      \"\"\"  dd \"\"\"\n",
    "      U = tf.random_uniform(shape,minval=0,maxval=1)\n",
    "      return -tf.log(-tf.log(U + eps) + eps)\n",
    "\n",
    "    def gumbel_softmax_sample(logits, temperature): \n",
    "      \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "      y = logits + GumbelSoftmax.sample_gumbel(tf.shape(logits))\n",
    "      return tf.nn.softmax( y / temperature)\n",
    "\n",
    "    def gumbel_softmax(logits, temperature, hard=False):\n",
    "      \"\"\"Sample from the Gumbel-Softmax distribution and optionally discretize.\n",
    "      Args:\n",
    "        logits: [batch_size, n_class] unnormalized log-probs\n",
    "        temperature: non-negative scalar\n",
    "        hard: if True, take argmax, but differentiate w.r.t. soft sample y\n",
    "      Returns:\n",
    "        [batch_size, n_class] sample from the Gumbel-Softmax distribution.\n",
    "        If hard=True, then the returned sample will be one-hot, otherwise it will\n",
    "        be a probabilitiy distribution that sums to 1 across classes\n",
    "      \"\"\"\n",
    "      y = GumbelSoftmax.gumbel_softmax_sample(logits, temperature)\n",
    "      if hard:\n",
    "        k = tf.shape(logits)[-1]\n",
    "        #y_hard = tf.cast(tf.one_hot(tf.argmax(y,1),k), y.dtype)\n",
    "        y_hard = tf.cast(tf.equal(y,tf.reduce_max(y,1,keep_dims=True)),y.dtype)\n",
    "        y = tf.stop_gradient(y_hard - y) + y\n",
    "      return y\n",
    "\n",
    "#sess = tf.Session()\n",
    "#sess.run(GumbelSoftmax.gumbel_softmax(tf.constant([[0.5, 0.5]]), 0.5, hard=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimulationPolicy(object):\n",
    "    \n",
    "    def __init__(self, obs_space, act_space, h_size=100):\n",
    "        print(\"Observation Space: \" , obs_space)\n",
    "        print(\"Action Space: \", act_space)\n",
    "        self.h_size = h_size\n",
    "        # Input space: [Episode_length, observations], output:[Episode_Length,action_space]\n",
    "        self.input = tf.placeholder(tf.float32, [None] + list(obs_space))\n",
    "        self.act_space = act_space\n",
    "        #self.output = self.getSoftAction(self.input)\n",
    "        self.sampleOutput = self.sample(self.input)\n",
    "        self.trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name) \n",
    "    \n",
    "    def getSoftAction(self,observation, reuse=None):\n",
    "        with tf.variable_scope(\"softaction\", reuse=reuse):\n",
    "            hidden = slim.fully_connected(observation, self.h_size, biases_initializer=None, activation_fn=tf.nn.relu)\n",
    "            return slim.fully_connected(hidden, self.act_space, activation_fn=tf.nn.softmax, biases_initializer=None)\n",
    "        \n",
    "    def sample(self,observation,temperature=0.5, reuse=None):\n",
    "        sess = tf.get_default_session()\n",
    "        #print(sess.run(tf.report_uninitialized_variables()))\n",
    "        softAction = self.getSoftAction(observation, reuse=reuse)\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        return GumbelSoftmax.gumbel_softmax(softAction, temperature, hard=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space:  (2,)\n",
      "Action Space:  3\n",
      "[[ 1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# sess = tf.Session()\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    simpolicy = SimulationPolicy( np.shape([1,1]),3)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample = simpolicy.sample(tf.constant([[5.0,1.0]]), reuse=True) \n",
    "    sample1 = simpolicy.sample(tf.constant([[5.0,1.0]]), reuse=True)\n",
    "    \n",
    "    writer = tf.summary.FileWriter('logs', sess.graph)\n",
    "    x, _ = sess.run([sample, sample1])\n",
    "    #writer.close()\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    \n",
    "    def __init__(self, parent_node):\n",
    "        self.parent = parent_node\n",
    "        self.children = []\n",
    "        \n",
    "    def addChild(self,forwardModel,simPolicy):\n",
    "        raw_actio_probs = simPolicy\n",
    "        \n",
    "\n",
    "class Tree(object):\n",
    "    \n",
    "    def __init__(self,):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
