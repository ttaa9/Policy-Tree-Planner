{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np, numpy.random as npr, random as r\n",
    "import tensorflow as tf  \n",
    "from NavTask import NavigationTask\n",
    "import tensorflow.contrib.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(x, size, name, initializer=None, bias_init=0):\n",
    "    print(\"x shape\",x.get_shape()[1])\n",
    "    print(\"size\", size)\n",
    "    w = tf.get_variable(name + \"/w\", [x.get_shape()[1], size], initializer=initializer)\n",
    "    b = tf.get_variable(name + \"/b\", [size], initializer=tf.constant_initializer(bias_init))\n",
    "    return tf.matmul(x, w) + b\n",
    "\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer\n",
    "\n",
    "def categorical_sample(logits, d):\n",
    "    value = tf.squeeze(tf.multinomial(logits - tf.reduce_max(logits, [1], keep_dims=True), 1), [1])\n",
    "    return tf.one_hot(value, d)\n",
    "\n",
    "class LSTMPolicy(object):\n",
    "\n",
    "    def __init__(self, ob_space, ac_space):\n",
    "        \n",
    "        print(\"obs space\", ob_space)\n",
    "        # x is the observations/states for the length of the episode\n",
    "        self.x = x = tf.placeholder(tf.float32,[None] + list(ob_space), name=\"x\")\n",
    "        print(\"x shape\", x)\n",
    "        size = 256\n",
    "        \n",
    "        # introduce a \"fake\" batch dimension of 1 to do LSTM over time dim\n",
    "        x = tf.expand_dims(x, [0])\n",
    "        \n",
    "        print(\"x shape\", x)\n",
    "        lstm = rnn.BasicLSTMCell(size, state_is_tuple=True)\n",
    "        self.state_size = lstm.state_size\n",
    "\n",
    "        #Step size for truncated backprop using the ob_space, basically [batch_size]\n",
    "        self.step_size = step_size = tf.shape(self.x)[:1]\n",
    "        print(\"step_size\", step_size)\n",
    "        \n",
    "        # defining the cell state and output state of the LSTM\n",
    "        c_init = np.zeros((1, lstm.state_size.c), np.float32)\n",
    "        h_init = np.zeros((1, lstm.state_size.h), np.float32)\n",
    "        self.state_init = [c_init, h_init]\n",
    "        \n",
    "        #defining placeholders so that we can input during training and inference, Example: during rollout you want to input these values \n",
    "        c_in = tf.placeholder(tf.float32, [1, lstm.state_size.c], name='c_in')\n",
    "        h_in = tf.placeholder(tf.float32, [1, lstm.state_size.h], name='h_in')\n",
    "        self.state_in = [c_in, h_in]\n",
    "        \n",
    "        state_in = rnn.LSTMStateTuple(c_in, h_in)\n",
    "        \n",
    "        lstm_outputs, lstm_state = tf.nn.dynamic_rnn(\n",
    "            lstm, x, initial_state=state_in, sequence_length=step_size,\n",
    "            time_major=False)\n",
    "        lstm_c, lstm_h = lstm_state\n",
    "        \n",
    "        print(lstm_outputs)\n",
    "        x = tf.reshape(lstm_outputs, [-1, size])\n",
    "        print(\"x as output\", x)\n",
    "        \n",
    "        # vf == value-function?? is one-dimenstion, so basically value for the given state? \n",
    "        self.vf = tf.reshape(linear(x, 1, \"value\", normalized_columns_initializer(1.0)), [-1])\n",
    "        \n",
    "        # can be used to later to get the values \n",
    "        self.state_out = [lstm_c[:1, :], lstm_h[:1, :]]\n",
    "\n",
    "        # [0, :] means pick action of first state from batch. Hardcoded b/c\n",
    "        # batch=1 during rollout collection. Its not used during batch training.\n",
    "        \n",
    "        self.logits = linear(x, ac_space, \"action\", normalized_columns_initializer(0.01))\n",
    "        print(\"logits\", self.logits)\n",
    "        self.sample = categorical_sample(self.logits, ac_space)[0, :]\n",
    "        print(\"sample\", self.sample)\n",
    "        self.probs = tf.nn.softmax(self.logits, dim=-1)[0, :]\n",
    "        print(\"self.probs\", self.probs)\n",
    "        \n",
    "        # need to do this over all the actions in the time series for training \n",
    "        #self.log_prob = log_prob = tf.log(tf.nn.softmax(self.logits, dim=-1))\n",
    "        #print(log_prob)\n",
    "        self.log_prob = log_prob = tf.nn.log_softmax(self.logits,  dim=-1)\n",
    "        self.prob_tf = tf.nn.softmax(self.logits)\n",
    "        \n",
    "        # training part of graph\n",
    "        self.ac = tf.placeholder(tf.float32, [None, ac_space], name=\"ac\")\n",
    "        self.adv = tf.placeholder(tf.float32, [None], name=\"adv\")\n",
    "        \n",
    "        \n",
    "        # get log probs of actions from episode\n",
    "        # tf.range ==== baically log_proba [10(timeseteps) x 10]\n",
    "#         indices = tf.range(0, tf.shape(log_prob)[0]) * tf.shape(log_prob)[1] + self._acts # [1 actions]\n",
    "#         act_prob = tf.gather(tf.reshape(log_prob, [-1]), indices)\n",
    "        self.crossEntropy = tf.reduce_sum(log_prob * self.ac, 1)\n",
    "        self.entropy = entropy = - tf.reduce_mean(tf.reduce_sum(self.prob_tf  * log_prob, 1))\n",
    "        self.loss = -tf.reduce_mean(self.crossEntropy * self.adv) - 0.001 * entropy\n",
    "\n",
    "        # loss\n",
    "        #self.loss = tf.reduce_sum(tf.multiply(act_prob, self._advantages))\n",
    "\n",
    "        # update\n",
    "        optimizer = tf.train.AdamOptimizer(0.1)\n",
    "        self._train = optimizer.minimize(self.loss)\n",
    "\n",
    "        #self.var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n",
    "    \n",
    "    def get_initial_features(self):\n",
    "        # Call this function to get reseted lstm memory cells\n",
    "        return self.state_init\n",
    "\n",
    "    def act(self, ob, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run([self.sample, self.vf] + self.state_out,\n",
    "                        {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})\n",
    "        return sess.run(self.pred, {self.input:x})\n",
    "\n",
    "    def act_inference(self, ob, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run([self.probs, self.sample, self.vf] + self.state_out,\n",
    "                        {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})\n",
    "\n",
    "    def value(self, ob, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run(self.vf, {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})[0]\n",
    "\n",
    "    def train_step(self, obs, acts, advantages, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        batch_feed = { self.x: obs, self.ac: acts, self.adv: advantages, self.state_in[0]: c, self.state_in[1]: h}\n",
    "        return sess.run([self._train, self.loss], feed_dict=batch_feed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inference(agent, hparams):\n",
    "    #\"Runs one episode\"\n",
    "    episode_length = hparams['epiode_length']\n",
    "    env = NavigationTask(3,3)\n",
    "    #print(env.getStateRep())\n",
    "    #observation, reward, done = env.getStateRep(), 0, False \n",
    "    obs, acts, rews = [], [], []\n",
    "    \n",
    "    c, h = agent.get_initial_features()\n",
    "    \n",
    "    for i in range(0, episode_length): # TODO: episode length\n",
    "        state = env.getStateRep()\n",
    "        obs.append(state)\n",
    "        \n",
    "        actionProb, sampleAction , _ , c, h  = agent.act_inference(state, c, h)\n",
    "        # todo: action_probablity \n",
    "        #print(sampleAction)\n",
    "        action = actionProb.argmax()\n",
    "        sampleActionIndex = sampleAction.argmax()\n",
    "        #todo: convert onehot vector into a j index \n",
    "        env.performAction(sampleActionIndex)\n",
    "        newState  = env.getStateRep()\n",
    "        reward = env.getReward() \n",
    "        \n",
    "        acts.append(action)\n",
    "        rews.append(reward)\n",
    "    return obs, acts, rews  \n",
    "    \n",
    "\n",
    "def policyRollout(agent, hparams):\n",
    "    \n",
    "    #\"Runs one episode\"\n",
    "    episode_length = hparams['epiode_length']\n",
    "    env = NavigationTask(3,3)\n",
    "    #print(env.getStateRep())\n",
    "    #observation, reward, done = env.getStateRep(), 0, False \n",
    "    obs, acts, rews = [], [], []\n",
    "    \n",
    "    c, h = agent.get_initial_features()\n",
    "    \n",
    "    for i in range(0, episode_length): # TODO: episode length\n",
    "        state = env.getStateRep()\n",
    "        obs.append(state)\n",
    "        \n",
    "        actionProb, sampleAction , _ , c, h  = agent.act_inference(state, c, h)\n",
    "        # todo: action_probablity \n",
    "        #print(sampleAction)\n",
    "        action = actionProb.argmax()\n",
    "        sampleActionIndex = sampleAction.argmax()\n",
    "        #todo: convert onehot vector into a j index \n",
    "        #env.performAction(action)\n",
    "        env.performAction(sampleActionIndex)\n",
    "        newState  = env.getStateRep()\n",
    "        reward = env.getReward() \n",
    "        \n",
    "        #acts.append(tf.one_hot([action],10))\n",
    "        #actions not smapled\n",
    "#         values = [sampleActionIndex]\n",
    "#         acts.append(np.squeeze(np.eye( hparams['num_actions'])[values]))\n",
    "        acts.append(sampleAction)\n",
    "        \n",
    "        \n",
    "        rews.append(reward)\n",
    "    return obs, acts, rews  \n",
    "\n",
    "\n",
    "def process_rewards(rews):\n",
    "    \"\"\"Rewards -> Advantages for one episode. \"\"\"\n",
    "\n",
    "    # total reward: length of episode\n",
    "    return [len(rews)] * len(rews)\n",
    "\n",
    "def main():\n",
    "    # hyper parameters\n",
    "    env = NavigationTask()\n",
    "    input_size = np.shape(env.getStateRep())\n",
    "    hparams = {\n",
    "            'input_size': input_size,\n",
    "            'num_actions': 10,\n",
    "            'learning_rate': 0.1,\n",
    "            'epiode_length': 6\n",
    "    }\n",
    "\n",
    "    # environment params\n",
    "    eparams = {\n",
    "            'num_batches': 100,\n",
    "            'ep_per_batch': 10\n",
    "    }\n",
    "\n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "\n",
    "        pi = LSTMPolicy(hparams['input_size'], hparams['num_actions'])\n",
    "\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        num = 0\n",
    "        for batch in range(0, eparams['num_batches']):\n",
    "            \n",
    "\n",
    "            #print('=====\\nBATCH {}\\n===='.format(batch))\n",
    "\n",
    "            b_obs, b_acts, b_rews = [], [], []\n",
    "\n",
    "            for i in range(0, eparams['ep_per_batch']):\n",
    "                c, h = pi.get_initial_features()\n",
    "                obs, acts, rews = policyRollout(pi, hparams)\n",
    "                num += 1 if 1 in rews else 0\n",
    "                if 1 in rews:\n",
    "                    print(\"loss\",pi.train_step(obs, acts, rews, c, h))\n",
    "                advantages = process_rewards(rews)\n",
    "                #print(\"Observation\", obs)\n",
    "                #print(\"acts\", acts[0])\n",
    "                #print(\"rews\", rews)\n",
    "                #print(\"loss\",pi.train_step(obs, acts, rews, c, h))\n",
    "                #value = pi.train_step(obs, acts, advantages, c, h)\n",
    "#             print(\"loss\",pi.train_step(obs, acts, rews, c, h))    \n",
    "            #_, loss = pi.train_step(obs, acts, rews, c, h)\n",
    "        print(inference(pi, hparams))\n",
    "#         print(\"loss\", loss)\n",
    "        print(num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs space (6,)\n",
      "x shape Tensor(\"x:0\", shape=(?, 6), dtype=float32)\n",
      "x shape Tensor(\"ExpandDims:0\", shape=(1, ?, 6), dtype=float32)\n",
      "step_size Tensor(\"strided_slice:0\", shape=(1,), dtype=int32)\n",
      "Tensor(\"rnn/transpose:0\", shape=(1, ?, 256), dtype=float32)\n",
      "x as output Tensor(\"Reshape:0\", shape=(?, 256), dtype=float32)\n",
      "x shape 256\n",
      "size 1\n",
      "x shape 256\n",
      "size 10\n",
      "logits Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n",
      "sample Tensor(\"strided_slice_3:0\", shape=(10,), dtype=float32)\n",
      "self.probs Tensor(\"strided_slice_4:0\", shape=(10,), dtype=float32)\n",
      "WARNING:tensorflow:From C:\\Users\\adity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "loss [None, 1.5326692]\n",
      "loss [None, 0.29812384]\n",
      "loss [None, 0.29669872]\n",
      "([array([ 0.,  0.,  1.,  0.,  0.,  0.]), array([ 0.,  2.,  1.,  0.,  0.,  0.]), array([ 0.,  2.,  1.,  0.,  0.,  0.]), array([ 0.,  2.,  1.,  0.,  0.,  0.]), array([ 0.,  2.,  1.,  0.,  0.,  0.]), array([ 0.,  2.,  1.,  0.,  0.,  0.])], [9, 9, 9, 9, 9, 9], [0, 0, 0, 0, 0, 0])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = [3]\n",
    "n_values = np.max(values) + 1\n",
    "np.squeeze(np.eye(10)[values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
