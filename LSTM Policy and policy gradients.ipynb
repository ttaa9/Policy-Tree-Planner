{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np, numpy.random as npr, random as r\n",
    "import tensorflow as tf  \n",
    "from NavTask import NavigationTask\n",
    "import tensorflow.contrib.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(x, size, name, initializer=None, bias_init=0):\n",
    "    print(\"x shape\",x.get_shape()[1])\n",
    "    print(\"size\", size)\n",
    "    w = tf.get_variable(name + \"/w\", [x.get_shape()[1], size], initializer=initializer)\n",
    "    b = tf.get_variable(name + \"/b\", [size], initializer=tf.constant_initializer(bias_init))\n",
    "    return tf.matmul(x, w) + b\n",
    "\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer\n",
    "\n",
    "def categorical_sample(logits, d):\n",
    "    value = tf.squeeze(tf.multinomial(logits - tf.reduce_max(logits, [1], keep_dims=True), 1), [1])\n",
    "    return tf.one_hot(value, d)\n",
    "\n",
    "class LSTMPolicy(object):\n",
    "\n",
    "    def __init__(self, ob_space, ac_space):\n",
    "        \n",
    "        print(\"obs space\", ob_space)\n",
    "        # x is the observations/states for the length of the episode  \n",
    "        self.x = x = tf.placeholder(tf.float32,[None] + list(ob_space), name=\"x\")\n",
    "        print(\"x shape\", x)\n",
    "        size = 256\n",
    "        \n",
    "        # introduce a \"fake\" batch dimension of 1 to do LSTM over time dim\n",
    "        x = tf.expand_dims(x, [0])\n",
    "        \n",
    "        print(\"x shape\", x)\n",
    "        lstm = rnn.BasicLSTMCell(size, state_is_tuple=True)\n",
    "        self.state_size = lstm.state_size\n",
    "\n",
    "        #Step size for truncated backprop using the ob_space, basically [batch_size]\n",
    "        step_size = tf.shape(self.x)[:1]\n",
    "        print(\"step_size\", step_size)\n",
    "        \n",
    "        # defining the cell state and output state of the LSTM\n",
    "        c_init = np.zeros((1, lstm.state_size.c), np.float32)\n",
    "        h_init = np.zeros((1, lstm.state_size.h), np.float32)\n",
    "        self.state_init = [c_init, h_init]\n",
    "        \n",
    "        #defining placeholders so that we can input during training and inference, Example: during rollout you want to input these values \n",
    "        c_in = tf.placeholder(tf.float32, [1, lstm.state_size.c], name='c_in')\n",
    "        h_in = tf.placeholder(tf.float32, [1, lstm.state_size.h], name='h_in')\n",
    "        self.state_in = [c_in, h_in]\n",
    "        \n",
    "        state_in = rnn.LSTMStateTuple(c_in, h_in)\n",
    "        \n",
    "        lstm_outputs, lstm_state = tf.nn.dynamic_rnn(\n",
    "            lstm, x, initial_state=state_in, sequence_length=step_size,\n",
    "            time_major=False)\n",
    "        lstm_c, lstm_h = lstm_state\n",
    "        \n",
    "        print(lstm_outputs)\n",
    "        x = tf.reshape(lstm_outputs, [-1, size])\n",
    "        print(\"x as output\", x)\n",
    "        \n",
    "        # vf == value-function?? is one-dimenstion, so basically value for the given state? \n",
    "        self.vf = tf.reshape(linear(x, 1, \"value\", normalized_columns_initializer(1.0)), [-1])\n",
    "        \n",
    "        # can be used to later to get the values \n",
    "        self.state_out = [lstm_c[:1, :], lstm_h[:1, :]]\n",
    "\n",
    "        # [0, :] means pick action of first state from batch. Hardcoded b/c\n",
    "        # batch=1 during rollout collection. Its not used during batch training.\n",
    "        \n",
    "        self.logits = linear(x, ac_space, \"action\", normalized_columns_initializer(0.01))\n",
    "        print(\"logits\", self.logits)\n",
    "        self.sample = categorical_sample(self.logits, ac_space)[0, :]\n",
    "        print(\"sample\", self.sample)\n",
    "        self.probs = tf.nn.softmax(self.logits, dim=-1)[0, :]\n",
    "        print(\"self.probs\", self.probs)\n",
    "        \n",
    "        # need to do this over all the actions in the time series for training \n",
    "        self.log_prob = log_prob = tf.log(tf.nn.softmax(self.logits, dim=-1))\n",
    "        print(log_prob)\n",
    "        \n",
    "        # training part of graph\n",
    "        self._acts = tf.placeholder(tf.int32)\n",
    "        self._advantages = tf.placeholder(tf.float32)\n",
    "        \n",
    "        \n",
    "        # get log probs of actions from episode\n",
    "        # tf.range ==== bsaically \n",
    "        indices = tf.range(0, tf.shape(log_prob)[0]) * tf.shape(log_prob)[1] + self._acts\n",
    "        act_prob = tf.gather(tf.reshape(log_prob, [-1]), indices)\n",
    "\n",
    "        # surrogate loss\n",
    "        self.loss = -tf.reduce_sum(tf.multiply(act_prob, self._advantages))\n",
    "\n",
    "        # update\n",
    "        optimizer = tf.train.RMSPropOptimizer(0.1)\n",
    "        self._train = optimizer.minimize(self.loss)\n",
    "\n",
    "        #self.var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n",
    "    \n",
    "    def get_initial_features(self):\n",
    "        # Call this function to get reseted lstm memory cells\n",
    "        return self.state_init\n",
    "\n",
    "    def act(self, ob, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run([self.sample, self.vf] + self.state_out,\n",
    "                        {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})\n",
    "\n",
    "    def act_inference(self, ob, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run([self.probs, self.sample, self.vf] + self.state_out,\n",
    "                        {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})\n",
    "\n",
    "    def value(self, ob, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run(self.vf, {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})[0]\n",
    "\n",
    "    def train_step(self, obs, acts, advantages, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        batch_feed = { self.x: obs, self._acts: acts, self._advantages: advantages, self.state_in[0]: c, self.state_in[1]: h}\n",
    "        return sess.run([self._train, self.loss], feed_dict=batch_feed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policyRollout(agent, hparams):\n",
    "    #\"Runs one episode\"\n",
    "    episode = hparams['epiode_length']\n",
    "    env = NavigationTask()\n",
    "    observation, reward, done = env.getStateRep(), 0, False \n",
    "    obs, acts, rews = [], [], []\n",
    "    \n",
    "    c, h = agent.get_initial_features()\n",
    "    \n",
    "    for i in range(0, episode): # TODO: batch length\n",
    "        state = env.getStateRep()\n",
    "        obs.append(state)\n",
    "        \n",
    "        actionProb, _ , _ , c, h  = agent.act_inference(state, c, h)\n",
    "        # todo: action_probablity \n",
    "        action = actionProb.argmax()\n",
    "        #todo: convert onehot vector into a j index \n",
    "        env.performAction(action)\n",
    "        newState  = env.getStateRep()\n",
    "        reward = env.getReward() \n",
    "        \n",
    "        acts.append(action)\n",
    "        rews.append(reward)\n",
    "    return obs, acts, rews  \n",
    "\n",
    "\n",
    "def process_rewards(rews):\n",
    "    \"\"\"Rewards -> Advantages for one episode. \"\"\"\n",
    "\n",
    "    # total reward: length of episode\n",
    "    return [len(rews)] * len(rews)\n",
    "\n",
    "def main():\n",
    "    # hyper parameters\n",
    "    env = NavigationTask()\n",
    "    input_size = np.shape(env.getStateRep())\n",
    "    hparams = {\n",
    "            'input_size': input_size,\n",
    "            'num_actions': 10,\n",
    "            'learning_rate': 0.1,\n",
    "            'epiode_length': 10\n",
    "    }\n",
    "\n",
    "    # environment params\n",
    "    eparams = {\n",
    "            'num_batches': 5000,\n",
    "            'ep_per_batch': 10\n",
    "    }\n",
    "\n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "\n",
    "        pi = LSTMPolicy(hparams['input_size'], hparams['num_actions'])\n",
    "\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        for batch in range(0, eparams['num_batches']):\n",
    "\n",
    "            print('=====\\nBATCH {}\\n===='.format(batch))\n",
    "\n",
    "            b_obs, b_acts, b_rews = [], [], []\n",
    "\n",
    "            for i in range(0, eparams['ep_per_batch']):\n",
    "                c, h = pi.get_initial_features()\n",
    "                obs, acts, rews = policyRollout(pi, hparams)\n",
    "                advantages = process_rewards(rews)\n",
    "                value = pi.train_step(obs, acts, advantages, c, h)\n",
    "                \n",
    "            _, loss = pi.train_step(obs, acts, rews, c, h)\n",
    "            print(\"loss\", loss)\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs space (6,)\n",
      "x shape Tensor(\"x:0\", shape=(?, 6), dtype=float32)\n",
      "x shape Tensor(\"ExpandDims:0\", shape=(1, ?, 6), dtype=float32)\n",
      "step_size Tensor(\"strided_slice:0\", shape=(1,), dtype=int32)\n",
      "Tensor(\"rnn/transpose:0\", shape=(1, ?, 256), dtype=float32)\n",
      "x as output Tensor(\"Reshape:0\", shape=(?, 256), dtype=float32)\n",
      "x shape 256\n",
      "size 1\n",
      "x shape 256\n",
      "size 10\n",
      "logits Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n",
      "sample Tensor(\"strided_slice_3:0\", shape=(10,), dtype=float32)\n",
      "self.probs Tensor(\"strided_slice_4:0\", shape=(10,), dtype=float32)\n",
      "Tensor(\"Log:0\", shape=(?, 10), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\adity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "=====\n",
      "BATCH 0\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 1\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 2\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 3\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 4\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 5\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 6\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 7\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 8\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 9\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 10\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 11\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 12\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 13\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 14\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 15\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 16\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 17\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 18\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 19\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 20\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 21\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 22\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 23\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 24\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 25\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 26\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 27\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 28\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 29\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 30\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 31\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 32\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 33\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 34\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 35\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 36\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 37\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 38\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 39\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 40\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 41\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 42\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 43\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 44\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 45\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 46\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 47\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 48\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 49\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 50\n",
      "====\n",
      "loss -0.0\n",
      "=====\n",
      "BATCH 51\n",
      "====\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-77b615a83887>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ep_per_batch'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_initial_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicyRollout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0madvantages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_rewards\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madvantages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-77b615a83887>\u001b[0m in \u001b[0;36mpolicyRollout\u001b[0;34m(agent, hparams)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mactionProb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact_inference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[1;31m# todo: action_probablity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactionProb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-1fcbd87b0a36>\u001b[0m in \u001b[0;36mact_inference\u001b[0;34m(self, ob, c, h)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         return sess.run([self.probs, self.sample, self.vf] + self.state_out,\n\u001b[0;32m--> 112\u001b[0;31m                         {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\adity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\adity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\adity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\adity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\adity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
