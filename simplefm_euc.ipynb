{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/vivonasg/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from SimpleTask import SimpleGridTask\n",
    "import numpy as np, numpy.random as npr, random as r, SimpleTask\n",
    "from TransportTask import TransportTask\n",
    "from NavTask import NavigationTask\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import os\n",
    "import time\n",
    "from SeqData import SeqData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _onenHotToEuc(batch_x,batch_y,env):\n",
    "    oriens_vec={'N':[1,0],'S':[-1,0],'E':[0,1],'W':[0,-1]}\n",
    "    oriens = ['N', 'E', 'S', 'W'] # 0,1,2,3\\n\",\n",
    "\n",
    "    batch_x=list(batch_x)\n",
    "    batch_y=list(batch_y)\n",
    "\n",
    "    #print(batch_x[:2])\\n\",\n",
    "    for as_init,s_fin,i in zip(batch_x,batch_y,range(len(batch_x))):\n",
    "\n",
    "        onehot_as_init =np.array(env.deconcatenateOneHotStateVector(as_init))\n",
    "\n",
    "        int_as_init=[env._oneHotToInt(onehot_as_init[i]) for i in range(0,len(onehot_as_init))]\\\n",
    "                       +[list(as_init[-10:])]\n",
    "        #int_as_init[2]=oriens_vec[oriens[int_as_init[2]]]\\n\",\n",
    "        int_as_init[2]=list(onehot_as_init[2])\n",
    "\n",
    "        onehot_s_fin=np.array(env.deconcatenateOneHotStateVector(s_fin))\n",
    "\n",
    "        int_s_fin=[env._oneHotToInt(onehot_s_fin[i]) for i in range(0,len(onehot_s_fin))]\\\n",
    "                       +[list(s_fin[-10:])]\n",
    "\n",
    "        #int_s_fin[2]=oriens_vec[oriens[int_s_fin[2]]]\\n\",\n",
    "        int_s_fin[2]=list(onehot_s_fin[2])\n",
    "        int_as_init=int_as_init[:2]+[env._oneHotToInt(int_as_init[2])]+int_as_init[3:-1]+int_as_init[-1]\n",
    "        int_s_fin=int_s_fin[:2]+[env._oneHotToInt(int_s_fin[2])]+int_s_fin[3:-1] #no action space\\n\",\n",
    "        \n",
    "        batch_x[i],batch_y[i]=[np.array(int_as_init),np.array(int_s_fin)]\n",
    "\n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g\n"
     ]
    }
   ],
   "source": [
    "class ForwardModelDifferentLoss_Euc():\n",
    "    def __init__(self, \n",
    "                obs_space, \n",
    "                input_space,\n",
    "                n_hidden=100\n",
    "                ):\n",
    "        self.n_hidden=n_hidden\n",
    "        self.act_space=input_space-obs_space\n",
    "        self.obs_space=obs_space\n",
    "        #Placeholders \n",
    "        self.input = tf.placeholder(\"float\", [None, input_space])\n",
    "        self.truevalue = tf.placeholder(\"float\", [None, obs_space])\n",
    "        self.pred=self.build_graph(self.input)\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        \n",
    "    def loss_function(self,batch_size,env):\n",
    "        accTotal=0\n",
    "        cost=0\n",
    "        for i in range(0,batch_size):\n",
    "            predVecs = env.deconcatenateOneHotStateVector(self.pred[i,:],w=1,h=1,no=1)\n",
    "            labelVecs = env.deconcatenateOneHotStateVector(self.truevalue[i,:],w=1,h=1,no=1)\n",
    "            for pv,lv,i in zip(predVecs,labelVecs,range(len(predVecs))):\n",
    "                cost +=  tf.reduce_mean(tf.pow(self.pred - self.truevalue, 2))\n",
    "                if i!=2:\n",
    "                    pv=tf.one_hot(tf.cast(pv,tf.int32),env.w)\n",
    "                    lv=tf.one_hot(tf.cast(lv,tf.int32),env.w)\n",
    "                \n",
    "                    pv=tf.reshape(pv,[env.w,])\n",
    "                    lv=tf.reshape(lv,[env.w,])\n",
    "                else:\n",
    "                    pv=tf.one_hot(tf.cast(pv,tf.int32),len(env.oriens))\n",
    "                    lv=tf.one_hot(tf.cast(lv,tf.int32),len(env.oriens))\n",
    "                \n",
    "                    pv=tf.reshape(pv,[len(env.oriens),])\n",
    "                    lv=tf.reshape(lv,[len(env.oriens),])\n",
    "                    \n",
    "                accTotal += tf.cast(tf.equal(tf.argmax(pv,axis=0), tf.argmax(lv,axis=0)), tf.float32)\n",
    "        \n",
    "        \n",
    "        print(cost)\n",
    "        print(accTotal)\n",
    "        return cost,accTotal\n",
    "\n",
    "    \n",
    "    def build_graph(self,inputVec, reuse=None):\n",
    "        with tf.variable_scope(\"forward-model\", reuse=reuse):\n",
    "            hidden = slim.fully_connected(inputVec, self.n_hidden, biases_initializer=None, activation_fn=tf.nn.relu)\n",
    "            hidden2 = slim.fully_connected(hidden, self.n_hidden, biases_initializer=None, activation_fn=tf.nn.relu)\n",
    "            return slim.fully_connected(hidden2,self.obs_space, activation_fn=None, biases_initializer=None)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        sess= tf.get_default_session()\n",
    "        #x.shape = (1,n_steps, n_input)\n",
    "        return sess.run([self.pred], {self.input:x})\n",
    "\n",
    "    def load_model(self,model_file_name):\n",
    "        sess= tf.get_default_session()\n",
    "        self.saver.restore(sess, model_file_name)\n",
    "\n",
    "    def train(self,trainset,testset,training_steps,batch_size,env,learning_rate,display_step, model_file_name=\"FWR_model_\"+time.strftime(\"%Y%m%d-%H%M%S\")):\n",
    "        sess= tf.get_default_session()\n",
    "        print('Entering loss func')\n",
    "        cost,accTotal = self.loss_function(batch_size,env)\n",
    "        print('Defining optimizer')\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "        self.accuracy = accTotal / (batch_size * trainset.env.stateSubVectors) #tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        # Initialize the variables (i.e. assign their default value)\n",
    "        print('Running TF initializer')\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        noise_sigma = 0.2\n",
    "        print('Entering train loop')\n",
    "        for step in range(1, training_steps + 1):\n",
    "            batch_x, batch_y = trainset.next_batch_nonseq(batch_size)\n",
    "            batch_x,batch_y= _onenHotToEuc(batch_x,batch_y,env)\n",
    "            npbx = np.array( batch_x )\n",
    "            npbxs = npbx.shape\n",
    "            noise = noise_sigma * np.random.randn( npbxs[0], npbxs[1] )\n",
    "            batch_x += noise\n",
    "            sess.run(self.optimizer, feed_dict={self.input: batch_x, self.truevalue: batch_y})\n",
    "            if step % display_step == 0 or step == 1:\n",
    "                # Calculate batch accuracy & loss\n",
    "                acc, loss = sess.run([self.accuracy, cost], feed_dict={self.input: batch_x, self.truevalue: batch_y})\n",
    "                print(\"Step \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.5f}\".format(acc))\n",
    "        print(\"Optimization Finished!\")\n",
    "        # Calculate accuracy\n",
    "        test_data, test_label = testset.next_batch_nonseq(5000) \n",
    "        test_data,test_label= _onenHotToEuc(batch_x,batch_y,env)\n",
    "        acc=sess.run(self.accuracy, feed_dict={self.input: test_data, self.truevalue: test_label})\n",
    "        print(\"Testing Accuracy:\",acc)\n",
    "        save_path= self.saver.save(sess, \"./\"+model_file_name+\".ckpt\")\n",
    "        print(\"Model Saved\")\n",
    "        return acc\n",
    "print('g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data\n",
      "Reading navigation-data-train-small.pickle\n",
      "\tBuilt\n",
      "Reading navigation-data-test-small.pickle\n",
      "\tBuilt\n",
      "5\n",
      "Defining Model\n",
      "Initializing FM\n",
      "FM initialized\n",
      "Entering loss func\n",
      "Tensor(\"add_638:0\", shape=(), dtype=float32)\n",
      "Tensor(\"add_639:0\", shape=(), dtype=float32)\n",
      "Defining optimizer\n",
      "Running TF initializer\n",
      "Entering train loop\n",
      "Step 64, Minibatch Loss= 21141.644531, Training Accuracy= 0.10938\n",
      "Step 12800, Minibatch Loss= 436.631012, Training Accuracy= 0.36875\n",
      "Step 25600, Minibatch Loss= 274.009399, Training Accuracy= 0.41563\n",
      "Step 38400, Minibatch Loss= 249.486343, Training Accuracy= 0.42812\n",
      "Step 51200, Minibatch Loss= 325.420837, Training Accuracy= 0.48750\n",
      "Step 64000, Minibatch Loss= 298.133972, Training Accuracy= 0.40938\n",
      "Step 76800, Minibatch Loss= 261.610199, Training Accuracy= 0.49062\n",
      "Step 89600, Minibatch Loss= 287.184021, Training Accuracy= 0.40938\n",
      "Step 102400, Minibatch Loss= 258.407898, Training Accuracy= 0.45312\n",
      "Step 115200, Minibatch Loss= 293.116058, Training Accuracy= 0.40000\n",
      "Step 128000, Minibatch Loss= 295.087616, Training Accuracy= 0.45937\n",
      "Step 140800, Minibatch Loss= 185.761292, Training Accuracy= 0.43438\n",
      "Step 153600, Minibatch Loss= 221.019104, Training Accuracy= 0.47813\n",
      "Step 166400, Minibatch Loss= 197.723053, Training Accuracy= 0.46250\n",
      "Step 179200, Minibatch Loss= 178.109909, Training Accuracy= 0.54063\n",
      "Step 192000, Minibatch Loss= 281.937744, Training Accuracy= 0.48438\n",
      "Step 204800, Minibatch Loss= 200.550278, Training Accuracy= 0.43750\n",
      "Step 217600, Minibatch Loss= 166.168488, Training Accuracy= 0.44688\n",
      "Step 230400, Minibatch Loss= 163.999054, Training Accuracy= 0.50313\n",
      "Step 243200, Minibatch Loss= 197.841415, Training Accuracy= 0.46875\n",
      "Step 256000, Minibatch Loss= 158.699585, Training Accuracy= 0.50625\n",
      "Step 268800, Minibatch Loss= 165.805573, Training Accuracy= 0.50938\n",
      "Step 281600, Minibatch Loss= 215.716660, Training Accuracy= 0.47500\n",
      "Step 294400, Minibatch Loss= 242.220627, Training Accuracy= 0.46875\n",
      "Step 307200, Minibatch Loss= 126.607025, Training Accuracy= 0.46563\n",
      "Step 320000, Minibatch Loss= 99.221237, Training Accuracy= 0.54063\n",
      "Step 332800, Minibatch Loss= 147.630249, Training Accuracy= 0.45000\n",
      "Step 345600, Minibatch Loss= 189.630234, Training Accuracy= 0.45000\n",
      "Step 358400, Minibatch Loss= 165.848190, Training Accuracy= 0.48438\n",
      "Step 371200, Minibatch Loss= 157.016220, Training Accuracy= 0.50000\n",
      "Step 384000, Minibatch Loss= 212.627029, Training Accuracy= 0.48438\n",
      "Step 396800, Minibatch Loss= 120.778931, Training Accuracy= 0.52188\n",
      "Step 409600, Minibatch Loss= 160.059052, Training Accuracy= 0.43125\n",
      "Step 422400, Minibatch Loss= 115.940575, Training Accuracy= 0.51562\n",
      "Step 435200, Minibatch Loss= 177.915009, Training Accuracy= 0.49375\n",
      "Step 448000, Minibatch Loss= 161.582428, Training Accuracy= 0.52812\n",
      "Step 460800, Minibatch Loss= 156.155716, Training Accuracy= 0.47813\n",
      "Step 473600, Minibatch Loss= 149.809967, Training Accuracy= 0.49375\n",
      "Step 486400, Minibatch Loss= 192.418289, Training Accuracy= 0.46875\n",
      "Step 499200, Minibatch Loss= 185.682007, Training Accuracy= 0.52812\n",
      "Step 512000, Minibatch Loss= 168.595596, Training Accuracy= 0.50000\n",
      "Step 524800, Minibatch Loss= 143.291428, Training Accuracy= 0.48750\n",
      "Step 537600, Minibatch Loss= 112.312149, Training Accuracy= 0.42188\n",
      "Step 550400, Minibatch Loss= 130.453613, Training Accuracy= 0.49062\n",
      "Step 563200, Minibatch Loss= 109.700409, Training Accuracy= 0.51562\n",
      "Step 576000, Minibatch Loss= 131.960999, Training Accuracy= 0.50938\n",
      "Step 588800, Minibatch Loss= 146.367294, Training Accuracy= 0.50000\n",
      "Step 601600, Minibatch Loss= 157.622467, Training Accuracy= 0.49688\n",
      "Step 614400, Minibatch Loss= 168.082458, Training Accuracy= 0.48125\n",
      "Step 627200, Minibatch Loss= 141.503494, Training Accuracy= 0.48438\n",
      "Step 640000, Minibatch Loss= 136.723083, Training Accuracy= 0.46875\n",
      "Step 652800, Minibatch Loss= 197.519455, Training Accuracy= 0.47188\n",
      "Step 665600, Minibatch Loss= 77.564926, Training Accuracy= 0.49062\n",
      "Step 678400, Minibatch Loss= 131.836533, Training Accuracy= 0.45000\n",
      "Step 691200, Minibatch Loss= 123.997093, Training Accuracy= 0.48438\n",
      "Step 704000, Minibatch Loss= 163.603149, Training Accuracy= 0.51562\n",
      "Step 716800, Minibatch Loss= 77.832794, Training Accuracy= 0.45625\n",
      "Step 729600, Minibatch Loss= 126.008766, Training Accuracy= 0.45625\n",
      "Step 742400, Minibatch Loss= 117.017113, Training Accuracy= 0.48125\n",
      "Step 755200, Minibatch Loss= 102.352798, Training Accuracy= 0.54063\n",
      "Step 768000, Minibatch Loss= 160.836868, Training Accuracy= 0.40938\n",
      "Step 780800, Minibatch Loss= 64.152893, Training Accuracy= 0.53438\n",
      "Step 793600, Minibatch Loss= 87.202324, Training Accuracy= 0.49062\n",
      "Step 806400, Minibatch Loss= 118.187950, Training Accuracy= 0.52812\n",
      "Step 819200, Minibatch Loss= 91.005867, Training Accuracy= 0.52500\n",
      "Step 832000, Minibatch Loss= 93.855652, Training Accuracy= 0.51562\n",
      "Step 844800, Minibatch Loss= 142.301987, Training Accuracy= 0.41875\n",
      "Step 857600, Minibatch Loss= 91.617973, Training Accuracy= 0.52812\n",
      "Step 870400, Minibatch Loss= 108.190742, Training Accuracy= 0.48125\n",
      "Step 883200, Minibatch Loss= 95.116562, Training Accuracy= 0.45312\n",
      "Step 896000, Minibatch Loss= 106.150085, Training Accuracy= 0.50313\n",
      "Step 908800, Minibatch Loss= 97.795403, Training Accuracy= 0.50625\n",
      "Step 921600, Minibatch Loss= 106.833679, Training Accuracy= 0.47188\n",
      "Step 934400, Minibatch Loss= 113.166512, Training Accuracy= 0.53125\n",
      "Step 947200, Minibatch Loss= 118.978867, Training Accuracy= 0.57812\n",
      "Step 960000, Minibatch Loss= 97.341461, Training Accuracy= 0.55937\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "attempt to get argmax of an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8db8671f82ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mfm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mForwardModelDifferentLoss_Euc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FM initialized'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisplay_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"trained_model_3_noise_0_2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3a0f8a2fb925>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainset, testset, training_steps, batch_size, env, learning_rate, display_step, model_file_name)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Calculate accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch_nonseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0m_onenHotToEuc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruevalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testing Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-361e4ee3cd42>\u001b[0m in \u001b[0;36m_onenHotToEuc\u001b[0;34m(batch_x, batch_y, env)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0monehot_as_init\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeconcatenateOneHotStateVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mint_as_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_oneHotToInt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot_as_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot_as_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m                       \u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m#int_as_init[2]=oriens_vec[oriens[int_as_init[2]]]\\n\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mint_as_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot_as_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-361e4ee3cd42>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0monehot_as_init\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeconcatenateOneHotStateVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mint_as_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_oneHotToInt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot_as_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot_as_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m                       \u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m#int_as_init[2]=oriens_vec[oriens[int_as_init[2]]]\\n\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mint_as_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot_as_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MSCAC_FALL_2017/SEMESTER_2/CSC2547/Policy-Tree-Planner/SimpleTask.py\u001b[0m in \u001b[0;36m_oneHotToInt\u001b[0;34m(self, h)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Helpers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_intToOneHot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_oneHotToInt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_strToInt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactionDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     \"\"\"\n\u001b[0;32m--> 963\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: attempt to get argmax of an empty sequence"
     ]
    }
   ],
   "source": [
    "print('Reading Data')\n",
    "s = 'navigation' #'navigation'\n",
    "trainf, validf = s+\"-data-train-small.pickle\", s+\"-data-test-small.pickle\"\n",
    "train, test   = SeqData(trainf), SeqData(validf)\n",
    "\n",
    "# classType = NavigationTask if s == 'navigation' else TransportTask\n",
    "print(train.env.stateSubVectors)\n",
    "print('Defining Model')\n",
    "# Parameters\n",
    "learning_rate = 0.0005\n",
    "training_steps = 15000 #2000 # 10000\n",
    "batch_size = 64 #256 #128\n",
    "display_step = 200\n",
    "# Network Parameters\n",
    "n_hidden = 50 #128 #5*train.lenOfInput # hidden layer num of features\n",
    "len_state = train.lenOfState # linear sequence or not\n",
    "len_input = train.lenOfInput\n",
    "\n",
    "\n",
    "\n",
    "print('Initializing FM')\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    fm=ForwardModelDifferentLoss_Euc(5,15, n_hidden)\n",
    "    print('FM initialized')\n",
    "    fm.train(train,test,training_steps,batch_size,train.env,learning_rate,display_step,\"trained_model_3_noise_0_2\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
