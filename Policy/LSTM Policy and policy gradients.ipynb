{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np, numpy.random as npr, random as r\n",
    "import tensorflow as tf  \n",
    "from NavTask import NavigationTask\n",
    "import tensorflow.contrib.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(x, size, name, initializer=None, bias_init=0):\n",
    "    print(\"x shape\",x.get_shape()[1])\n",
    "    print(\"size\", size)\n",
    "    w = tf.get_variable(name + \"/w\", [x.get_shape()[1], size], initializer=initializer)\n",
    "    b = tf.get_variable(name + \"/b\", [size], initializer=tf.constant_initializer(bias_init))\n",
    "    return tf.matmul(x, w) + b\n",
    "\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer\n",
    "\n",
    "def categorical_sample(logits, d):\n",
    "    value = tf.squeeze(tf.multinomial(logits - tf.reduce_max(logits, [1], keep_dims=True), 1), [1])\n",
    "    return tf.one_hot(value, d)\n",
    "\n",
    "class LSTMPolicy(object):\n",
    "\n",
    "    def __init__(self, ob_space, ac_space):\n",
    "        \n",
    "        print(\"obs space\", ob_space)\n",
    "        # x is the observations/states for the length of the episode\n",
    "        self.x = x = tf.placeholder(tf.float32,[None] + list(ob_space), name=\"x\")\n",
    "        print(\"x shape\", x)\n",
    "        size = 256\n",
    "        \n",
    "        # introduce a \"fake\" batch dimension of 1 to do LSTM over time dim\n",
    "        x = tf.expand_dims(x, [0])\n",
    "        \n",
    "        print(\"x shape\", x)\n",
    "        lstm = rnn.BasicLSTMCell(size, state_is_tuple=True)\n",
    "        self.state_size = lstm.state_size\n",
    "\n",
    "        #Step size for truncated backprop using the ob_space, basically [batch_size]\n",
    "        self.step_size = step_size = tf.shape(self.x)[:1]\n",
    "        print(\"step_size\", step_size)\n",
    "        \n",
    "        # defining the cell state and output state of the LSTM\n",
    "        c_init = np.zeros((1, lstm.state_size.c), np.float32)\n",
    "        h_init = np.zeros((1, lstm.state_size.h), np.float32)\n",
    "        self.state_init = [c_init, h_init]\n",
    "        \n",
    "        #defining placeholders so that we can input during training and inference, Example: during rollout you want to input these values \n",
    "        c_in = tf.placeholder(tf.float32, [1, lstm.state_size.c], name='c_in')\n",
    "        h_in = tf.placeholder(tf.float32, [1, lstm.state_size.h], name='h_in')\n",
    "        self.state_in = [c_in, h_in]\n",
    "        \n",
    "        state_in = rnn.LSTMStateTuple(c_in, h_in)\n",
    "        \n",
    "        lstm_outputs, lstm_state = tf.nn.dynamic_rnn(\n",
    "            lstm, x, initial_state=state_in, sequence_length=step_size,\n",
    "            time_major=False)\n",
    "        lstm_c, lstm_h = lstm_state\n",
    "        \n",
    "        print(lstm_outputs)\n",
    "        x = tf.reshape(lstm_outputs, [-1, size])\n",
    "        print(\"x as output\", x)\n",
    "        \n",
    "        # vf == value-function?? is one-dimenstion, so basically value for the given state? \n",
    "        self.vf = tf.reshape(linear(x, 1, \"value\", normalized_columns_initializer(1.0)), [-1])\n",
    "        \n",
    "        # can be used to later to get the values \n",
    "        self.state_out = [lstm_c[:1, :], lstm_h[:1, :]]\n",
    "\n",
    "        # [0, :] means pick action of first state from batch. Hardcoded b/c\n",
    "        # batch=1 during rollout collection. Its not used during batch training.\n",
    "        \n",
    "        self.logits = linear(x, ac_space, \"action\", normalized_columns_initializer(0.01))\n",
    "        print(\"logits\", self.logits)\n",
    "        self.sample = categorical_sample(self.logits, ac_space)[0, :]\n",
    "        print(\"sample\", self.sample)\n",
    "        self.probs = tf.nn.softmax(self.logits, dim=-1)[0, :]\n",
    "        print(\"self.probs\", self.probs)\n",
    "        \n",
    "        # need to do this over all the actions in the time series for training \n",
    "        #self.log_prob = log_prob = tf.log(tf.nn.softmax(self.logits, dim=-1))\n",
    "        #print(log_prob)\n",
    "        self.log_prob = log_prob = tf.nn.log_softmax(self.logits,  dim=-1)\n",
    "        self.prob_tf = tf.nn.softmax(self.logits)\n",
    "        \n",
    "        # training part of graph\n",
    "        self.ac = tf.placeholder(tf.float32, [None, ac_space], name=\"ac\")\n",
    "        self.adv = tf.placeholder(tf.float32, [None], name=\"adv\")\n",
    "        self.cumaltiveReward = tf.reduce_sum(self.adv)\n",
    "        \n",
    "        # get log probs of actions from episode\n",
    "        # tf.range ==== baically log_proba [10(timeseteps) x 10]\n",
    "#         indices = tf.range(0, tf.shape(log_prob)[0]) * tf.shape(log_prob)[1] + self._acts # [1 actions]\n",
    "#         act_prob = tf.gather(tf.reshape(log_prob, [-1]), indices)\n",
    "#         self.crossEntropy = tf.reduce_sum(log_prob * self.ac, 1)\n",
    "#         self.cumaltive = tf.reduce_sum(self.adv)\n",
    "#         self.entropy = entropy = 0#- tf.reduce_mean(tf.reduce_sum(self.prob_tf  * log_prob, 1))\n",
    "#         self.loss = -tf.reduce_mean(self.crossEntropy * self.cumaltive) - 0.001 * entropy\n",
    "        self.cross_entropy_loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.log_prob, labels=self.ac)\n",
    "        self.loss = tf.reduce_mean(self.cross_entropy_loss)\n",
    "        self.gradients = tf.train.AdamOptimizer(0.1).compute_gradients(self.loss)\n",
    "        for i, (grad, var) in enumerate(self.gradients):\n",
    "            if grad is not None:\n",
    "                self.gradients[i] = (grad * self.cumaltiveReward, var)\n",
    "                \n",
    "        self._train = tf.train.AdamOptimizer(0.1).apply_gradients(self.gradients)\n",
    "        # loss\n",
    "        #self.loss = tf.reduce_sum(tf.multiply(act_prob, self._advantages))\n",
    "\n",
    "        # update\n",
    "#         optimizer = tf.train.AdamOptimizer(0.1)\n",
    "#         self._train = optimizer.minimize(self.loss)\n",
    "\n",
    "        #self.var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n",
    "    \n",
    "    def get_initial_features(self):\n",
    "        # Call this function to get reseted lstm memory cells\n",
    "        return self.state_init\n",
    "\n",
    "    def act(self, ob, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run([self.sample, self.vf] + self.state_out,\n",
    "                        {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})\n",
    "        return sess.run(self.pred, {self.input:x})\n",
    "\n",
    "    def act_inference(self, ob, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run([self.probs, self.sample, self.vf] + self.state_out,\n",
    "                        {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})\n",
    "\n",
    "    def value(self, ob, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run(self.vf, {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})[0]\n",
    "\n",
    "    def train_step(self, obs, acts, advantages, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        batch_feed = { self.x: obs, self.ac: acts, self.adv: advantages, self.state_in[0]: c, self.state_in[1]: h}\n",
    "        return sess.run([self._train, self.loss, self.cross_entropy_loss], feed_dict=batch_feed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inference(agent, hparams):\n",
    "    #\"Runs one episode\"\n",
    "    episode_length = hparams['epiode_length']\n",
    "    env = NavigationTask(3,3)\n",
    "    #print(env.getStateRep())\n",
    "    #observation, reward, done = env.getStateRep(), 0, False \n",
    "    obs, acts, rews = [], [], []\n",
    "    \n",
    "    c, h = agent.get_initial_features()\n",
    "    \n",
    "    for i in range(0, episode_length): # TODO: episode length\n",
    "        state = env.getStateRep()\n",
    "        obs.append(state)\n",
    "        \n",
    "        actionProb, sampleAction , _ , c, h  = agent.act_inference(state, c, h)\n",
    "        # todo: action_probablity \n",
    "        #print(sampleAction)\n",
    "        action = actionProb.argmax()\n",
    "        sampleActionIndex = sampleAction.argmax()\n",
    "        #todo: convert onehot vector into a j index \n",
    "        env.performAction(sampleActionIndex)\n",
    "        newState  = env.getStateRep()\n",
    "        reward = env.getReward() \n",
    "        if reward == 0:\n",
    "            reward = -0.1\n",
    "        acts.append(action)\n",
    "        rews.append(reward)\n",
    "    return obs, acts, rews  \n",
    "    \n",
    "\n",
    "def policyRollout(agent, hparams):\n",
    "    \n",
    "    #\"Runs one episode\"\n",
    "    episode_length = hparams['epiode_length']\n",
    "    env = NavigationTask(3,3)\n",
    "    #print(env.getStateRep())\n",
    "    #observation, reward, done = env.getStateRep(), 0, False \n",
    "    obs, acts, rews = [], [], []\n",
    "    \n",
    "    c, h = agent.get_initial_features()\n",
    "    \n",
    "    for i in range(0, episode_length): # TODO: episode length\n",
    "        state = env.getStateRep()\n",
    "        obs.append(state)\n",
    "        \n",
    "        actionProb, sampleAction , _ , c, h  = agent.act_inference(state, c, h)\n",
    "        # todo: action_probablity \n",
    "        #print(sampleAction)\n",
    "        action = actionProb.argmax()\n",
    "        sampleActionIndex = sampleAction.argmax()\n",
    "        #todo: convert onehot vector into a j index \n",
    "        #env.performAction(action)\n",
    "        env.performAction(sampleActionIndex)\n",
    "        newState  = env.getStateRep()\n",
    "        reward = env.getReward(distance_based=True) \n",
    "        if reward == 0:\n",
    "            reward = -0.1\n",
    "        #acts.append(tf.one_hot([action],10))\n",
    "        #actions not smapled\n",
    "#         values = [sampleActionIndex]\n",
    "#         acts.append(np.squeeze(np.eye( hparams['num_actions'])[values]))\n",
    "        acts.append(sampleAction)\n",
    "        \n",
    "        \n",
    "        rews.append(reward)\n",
    "    return obs, acts, rews  \n",
    "\n",
    "\n",
    "def process_rewards(rews):\n",
    "    \"\"\"Rewards -> Advantages for one episode. \"\"\"\n",
    "\n",
    "    # total reward: length of episode\n",
    "    return [len(rews)] * len(rews)\n",
    "\n",
    "def main():\n",
    "    # hyper parameters\n",
    "    env = NavigationTask()\n",
    "    input_size = np.shape(env.getStateRep())\n",
    "    hparams = {\n",
    "            'input_size': input_size,\n",
    "            'num_actions': 10,\n",
    "            'learning_rate': 0.1,\n",
    "            'epiode_length': 6\n",
    "    }\n",
    "\n",
    "    # environment params\n",
    "    eparams = {\n",
    "            'num_batches': 100,\n",
    "            'ep_per_batch': 100\n",
    "    }\n",
    "\n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "\n",
    "        pi = LSTMPolicy(hparams['input_size'], hparams['num_actions'])\n",
    "\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        num = 0\n",
    "        for batch in range(0, eparams['num_batches']):\n",
    "            \n",
    "\n",
    "            #print('=====\\nBATCH {}\\n===='.format(batch))\n",
    "\n",
    "            b_obs, b_acts, b_rews = [], [], []\n",
    "\n",
    "            for i in range(0, eparams['ep_per_batch']):\n",
    "                c, h = pi.get_initial_features()\n",
    "                obs, acts, rews = policyRollout(pi, hparams)\n",
    "                num += 1 if 1 in rews else 0\n",
    "                if 1. in rews:\n",
    "                    print(\"loss\",pi.train_step(obs, acts, rews, c, h))\n",
    "                    print(\"iteration\", i)\n",
    "                advantages = process_rewards(rews)\n",
    "                if False:#i%1000 == 0:\n",
    "                    print(\"Observation\", obs)\n",
    "                    print(\"acts\", [np.argmax(a) for a in acts])\n",
    "                    print(\"rews\", rews)\n",
    "#                 print(\"Observation\", obs)\n",
    "#                 print(\"acts\", [np.argmax(a) for a in acts])\n",
    "#                 print(\"rews\", rews)\n",
    "                #print(\"loss\",pi.train_step(obs, acts, rews, c, h))\n",
    "                #value = pi.train_step(obs, acts, advantages, c, h)\n",
    "#             print(\"loss\",pi.train_step(obs, acts, rews, c, h))    \n",
    "            #_, loss = pi.train_step(obs, acts, rews, c, h)\n",
    "        #print(inference(pi, hparams))\n",
    "#         print(\"loss\", loss)\n",
    "        print(num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs space (8,)\n",
      "x shape Tensor(\"x:0\", shape=(?, 8), dtype=float32)\n",
      "x shape Tensor(\"ExpandDims:0\", shape=(1, ?, 8), dtype=float32)\n",
      "step_size Tensor(\"strided_slice:0\", shape=(1,), dtype=int32)\n",
      "Tensor(\"rnn/transpose:0\", shape=(1, ?, 256), dtype=float32)\n",
      "x as output Tensor(\"Reshape:0\", shape=(?, 256), dtype=float32)\n",
      "x shape 256\n",
      "size 1\n",
      "x shape 256\n",
      "size 10\n",
      "logits Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n",
      "sample Tensor(\"strided_slice_3:0\", shape=(10,), dtype=float32)\n",
      "self.probs Tensor(\"strided_slice_4:0\", shape=(10,), dtype=float32)\n",
      "WARNING:tensorflow:From C:\\Users\\adity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "=====\n",
      "BATCH 0\n",
      "====\n",
      "loss [None, 2.3023655]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.])]\n",
      "acts [1, 8, 0, 8, 5, 5]\n",
      "rews [0.20000000000000001, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331]\n",
      "=====\n",
      "BATCH 1\n",
      "====\n",
      "loss [None, 0.9636361]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.])]\n",
      "acts [8, 8, 5, 8, 8, 5]\n",
      "rews [0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331]\n",
      "=====\n",
      "BATCH 2\n",
      "====\n",
      "loss [None, 0.00033462245]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.])]\n",
      "acts [8, 8, 8, 8, 8, 8]\n",
      "rews [0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331]\n",
      "=====\n",
      "BATCH 3\n",
      "====\n",
      "loss [None, 2.5828658e-07]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.])]\n",
      "acts [8, 8, 8, 8, 8, 8]\n",
      "rews [0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331]\n",
      "=====\n",
      "BATCH 4\n",
      "====\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-88dd433ed192>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ep_per_batch'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_initial_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicyRollout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0mnum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrews\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[1;31m#                 if 1. in rews:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-88dd433ed192>\u001b[0m in \u001b[0;36mpolicyRollout\u001b[0;34m(agent, hparams)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mactionProb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampleAction\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact_inference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[1;31m# todo: action_probablity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[1;31m#print(sampleAction)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-05bbcc5a1427>\u001b[0m in \u001b[0;36mact_inference\u001b[0;34m(self, ob, c, h)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         return sess.run([self.probs, self.sample, self.vf] + self.state_out,\n\u001b[0;32m--> 126\u001b[0;31m                         {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\adity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\adity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\adity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\adity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\adity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values = [3]\n",
    "n_values = np.max(values) + 1\n",
    "np.squeeze(np.eye(10)[values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main2():\n",
    "    # hyper parameters\n",
    "    env = NavigationTask()\n",
    "    input_size = np.shape(env.getStateRep())\n",
    "    hparams = {\n",
    "            'input_size': input_size,\n",
    "            'num_actions': 10,\n",
    "            'learning_rate': 0.1,\n",
    "            'epiode_length': 6\n",
    "    }\n",
    "\n",
    "    # environment params\n",
    "    eparams = {\n",
    "            'num_batches': 10,\n",
    "            'ep_per_batch': 500\n",
    "    }\n",
    "\n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "\n",
    "        pi = LSTMPolicy(hparams['input_size'], hparams['num_actions'])\n",
    "\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        num = 0\n",
    "        for batch in range(0, eparams['num_batches']):\n",
    "            \n",
    "\n",
    "            print('=====\\nBATCH {}\\n===='.format(batch))\n",
    "\n",
    "            #b_obs, b_acts, b_rews = [], [], []\n",
    "\n",
    "            for i in range(0, eparams['ep_per_batch']):\n",
    "                c, h = pi.get_initial_features()\n",
    "                obs, acts, rews = policyRollout(pi, hparams)\n",
    "                num += 1 if 1 in rews else 0\n",
    "                advantages = process_rewards(rews)\n",
    "                pi.train_step(obs, acts, rews, c, h)\n",
    "            c, h = pi.get_initial_features()\n",
    "            obs, acts, rews = policyRollout(pi, hparams)\n",
    "            print(\"loss\",pi.train_step(obs, acts, rews, c, h))\n",
    "            print(\"Observation\", obs)\n",
    "            print(\"acts\", [np.argmax(a) for a in acts])\n",
    "            print(\"rews\", rews)\n",
    "        print(inference(pi, hparams))\n",
    "        print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs space (8,)\n",
      "x shape Tensor(\"x:0\", shape=(?, 8), dtype=float32)\n",
      "x shape Tensor(\"ExpandDims:0\", shape=(1, ?, 8), dtype=float32)\n",
      "step_size Tensor(\"strided_slice:0\", shape=(1,), dtype=int32)\n",
      "Tensor(\"rnn/transpose:0\", shape=(1, ?, 256), dtype=float32)\n",
      "x as output Tensor(\"Reshape:0\", shape=(?, 256), dtype=float32)\n",
      "x shape 256\n",
      "size 1\n",
      "x shape 256\n",
      "size 10\n",
      "logits Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n",
      "sample Tensor(\"strided_slice_3:0\", shape=(10,), dtype=float32)\n",
      "self.probs Tensor(\"strided_slice_4:0\", shape=(10,), dtype=float32)\n",
      "WARNING:tensorflow:From C:\\Users\\adity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "=====\n",
      "BATCH 0\n",
      "====\n",
      "loss [None, 0.0]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.])]\n",
      "acts [3, 3, 3, 3, 3, 3]\n",
      "rews [0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001]\n",
      "=====\n",
      "BATCH 1\n",
      "====\n",
      "loss [None, 0.0]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.])]\n",
      "acts [3, 3, 3, 3, 3, 3]\n",
      "rews [0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001]\n",
      "=====\n",
      "BATCH 2\n",
      "====\n",
      "loss [None, 0.0]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.])]\n",
      "acts [3, 3, 3, 3, 3, 3]\n",
      "rews [0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001]\n",
      "=====\n",
      "BATCH 3\n",
      "====\n",
      "loss [None, 0.0]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.])]\n",
      "acts [3, 3, 3, 3, 3, 3]\n",
      "rews [0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001]\n",
      "=====\n",
      "BATCH 4\n",
      "====\n",
      "loss [None, 0.0]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.])]\n",
      "acts [3, 3, 3, 3, 3, 3]\n",
      "rews [0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001]\n",
      "=====\n",
      "BATCH 5\n",
      "====\n",
      "loss [None, 0.0]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.])]\n",
      "acts [3, 3, 3, 3, 3, 3]\n",
      "rews [0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001]\n",
      "=====\n",
      "BATCH 6\n",
      "====\n",
      "loss [None, 0.0]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.])]\n",
      "acts [3, 3, 3, 3, 3, 3]\n",
      "rews [0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001]\n",
      "=====\n",
      "BATCH 7\n",
      "====\n",
      "loss [None, 0.0]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.])]\n",
      "acts [3, 3, 3, 3, 3, 3]\n",
      "rews [0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001]\n",
      "=====\n",
      "BATCH 8\n",
      "====\n",
      "loss [None, 0.0]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.])]\n",
      "acts [3, 3, 3, 3, 3, 3]\n",
      "rews [0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001]\n",
      "=====\n",
      "BATCH 9\n",
      "====\n",
      "loss [None, 0.0]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.])]\n",
      "acts [3, 3, 3, 3, 3, 3]\n",
      "rews [0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001]\n",
      "([array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.])], [3, 3, 3, 3, 3, 3], [0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "main2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
