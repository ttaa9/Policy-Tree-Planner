{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np, numpy.random as npr, random as r\n",
    "import tensorflow as tf  \n",
    "from NavTask import NavigationTask\n",
    "import tensorflow.contrib.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(x, size, name, initializer=None, bias_init=0):\n",
    "    print(\"x shape\",x.get_shape()[1])\n",
    "    print(\"size\", size)\n",
    "    w = tf.get_variable(name + \"/w\", [x.get_shape()[1], size], initializer=initializer)\n",
    "    b = tf.get_variable(name + \"/b\", [size], initializer=tf.constant_initializer(bias_init))\n",
    "    return tf.matmul(x, w) + b\n",
    "\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer\n",
    "\n",
    "def categorical_sample(logits, d):\n",
    "    value = tf.squeeze(tf.multinomial(logits - tf.reduce_max(logits, [1], keep_dims=True), 1), [1])\n",
    "    return tf.one_hot(value, d)\n",
    "\n",
    "class LSTMPolicy(object):\n",
    "\n",
    "    def __init__(self, ob_space, ac_space):\n",
    "        \n",
    "        print(\"obs space\", ob_space)\n",
    "        # x is the observations/states for the length of the episode\n",
    "        self.x = x = tf.placeholder(tf.float32,[None] + list(ob_space), name=\"x\")\n",
    "        print(\"x shape\", x)\n",
    "        size = 256\n",
    "        \n",
    "        # introduce a \"fake\" batch dimension of 1 to do LSTM over time dim\n",
    "        x = tf.expand_dims(x, [0])\n",
    "        \n",
    "        print(\"x shape\", x)\n",
    "        lstm = rnn.BasicLSTMCell(size, state_is_tuple=True)\n",
    "        self.state_size = lstm.state_size\n",
    "\n",
    "        #Step size for truncated backprop using the ob_space, basically [batch_size]\n",
    "        self.step_size = step_size = tf.shape(self.x)[:1]\n",
    "        print(\"step_size\", step_size)\n",
    "        \n",
    "        # defining the cell state and output state of the LSTM\n",
    "        c_init = np.zeros((1, lstm.state_size.c), np.float32)\n",
    "        h_init = np.zeros((1, lstm.state_size.h), np.float32)\n",
    "        self.state_init = [c_init, h_init]\n",
    "        \n",
    "        #defining placeholders so that we can input during training and inference, Example: during rollout you want to input these values \n",
    "        c_in = tf.placeholder(tf.float32, [1, lstm.state_size.c], name='c_in')\n",
    "        h_in = tf.placeholder(tf.float32, [1, lstm.state_size.h], name='h_in')\n",
    "        self.state_in = [c_in, h_in]\n",
    "        \n",
    "        state_in = rnn.LSTMStateTuple(c_in, h_in)\n",
    "        \n",
    "        lstm_outputs, lstm_state = tf.nn.dynamic_rnn(\n",
    "            lstm, x, initial_state=state_in, sequence_length=step_size,\n",
    "            time_major=False)\n",
    "        lstm_c, lstm_h = lstm_state\n",
    "        \n",
    "        print(lstm_outputs)\n",
    "        x = tf.reshape(lstm_outputs, [-1, size])\n",
    "        print(\"x as output\", x)\n",
    "        \n",
    "        # vf == value-function?? is one-dimenstion, so basically value for the given state? \n",
    "        self.vf = tf.reshape(linear(x, 1, \"value\", normalized_columns_initializer(1.0)), [-1])\n",
    "        \n",
    "        # can be used to later to get the values \n",
    "        self.state_out = [lstm_c[:1, :], lstm_h[:1, :]]\n",
    "\n",
    "        # [0, :] means pick action of first state from batch. Hardcoded b/c\n",
    "        # batch=1 during rollout collection. Its not used during batch training.\n",
    "        \n",
    "        self.logits = linear(x, ac_space, \"action\", normalized_columns_initializer(0.01))\n",
    "        print(\"logits\", self.logits)\n",
    "        self.sample = categorical_sample(self.logits, ac_space)[0, :]\n",
    "        print(\"sample\", self.sample)\n",
    "        self.probs = tf.nn.softmax(self.logits, dim=-1)[0, :]\n",
    "        print(\"self.probs\", self.probs)\n",
    "        \n",
    "     \n",
    "        self.log_prob = log_prob = tf.nn.log_softmax(self.logits,  dim=-1)\n",
    "        self.prob_tf = tf.nn.softmax(self.logits)\n",
    "        \n",
    "        # training part of graph\n",
    "        self.ac = tf.placeholder(tf.float32, [None, ac_space], name=\"ac\")\n",
    "        self.adv = tf.placeholder(tf.float32, [None], name=\"adv\")\n",
    "        self.cumaltiveReward = tf.reduce_sum(self.adv)\n",
    "        self.entropy =  tf.reduce_mean(tf.reduce_sum(self.prob_tf  * log_prob, 1))\n",
    "  \n",
    "        self.cross_entropy_loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.log_prob, labels=self.ac)\n",
    "        self.loss =  tf.reduce_mean(self.cross_entropy_loss) + 0.1*self.entropy \n",
    "        self.gradients = tf.train.AdamOptimizer(0.001).compute_gradients(self.loss)\n",
    "        for i, (grad, var) in enumerate(self.gradients):\n",
    "            if grad is not None:\n",
    "                self.gradients[i] = (grad * self.cumaltiveReward, var)\n",
    "                \n",
    "        self._train = tf.train.AdamOptimizer(0.001).apply_gradients(self.gradients)\n",
    "       \n",
    "    \n",
    "    def get_initial_features(self):\n",
    "        # Call this function to get reseted lstm memory cells\n",
    "        return self.state_init\n",
    "\n",
    "    def act(self, ob, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run([self.sample, self.vf] + self.state_out,\n",
    "                        {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})\n",
    "        return sess.run(self.pred, {self.input:x})\n",
    "\n",
    "    def act_inference(self, ob, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run([self.probs, self.sample, self.vf] + self.state_out,\n",
    "                        {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})\n",
    "\n",
    "    def value(self, ob, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run(self.vf, {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})[0]\n",
    "\n",
    "    def train_step(self, obs, acts, advantages, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        batch_feed = { self.x: obs, self.ac: acts, self.adv: advantages, self.state_in[0]: c, self.state_in[1]: h}\n",
    "        return sess.run([self._train, self.loss, self.logits], feed_dict=batch_feed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(agent, hparams):\n",
    "    #\"Runs one episode\"\n",
    "    episode_length = hparams['epiode_length']\n",
    "    env = NavigationTask(5,5)\n",
    "    obs, acts, rews = [], [], []\n",
    "    c, h = agent.get_initial_features()\n",
    "    for i in range(0, episode_length): \n",
    "        state = env.getStateRep()\n",
    "        obs.append(state)\n",
    "        \n",
    "        actionProb, sampleAction , _ , c, h  = agent.act_inference(state, c, h)\n",
    "        \n",
    "        action = actionProb.argmax()\n",
    "        sampleActionIndex = sampleAction.argmax()\n",
    "        \n",
    "        env.performAction(action)\n",
    "        newState  = env.getStateRep()\n",
    "        reward = env.getReward() \n",
    "        \n",
    "        acts.append(action)\n",
    "        rews.append(reward)\n",
    "        \n",
    "    return obs, acts, rews  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policyRollout(agent, hparams):\n",
    "    \n",
    "    #\"Runs one episode\"\n",
    "    episode_length = hparams['epiode_length']\n",
    "    env = NavigationTask(5,5)\n",
    "    obs, acts, rews = [], [], []\n",
    "    c, h = agent.get_initial_features()\n",
    "    \n",
    "    for i in range(0, episode_length): \n",
    "        \n",
    "        state = env.getStateRep()\n",
    "        obs.append(state)\n",
    "        actionProb, sampleAction , _ , c, h  = agent.act_inference(state, c, h)\n",
    "      \n",
    "        action = actionProb.argmax()\n",
    "        sampleActionIndex = sampleAction.argmax()\n",
    "        \n",
    "        env.performAction(sampleActionIndex)\n",
    "        newState  = env.getStateRep()\n",
    "        reward = env.getReward() \n",
    "        \n",
    "        acts.append(sampleAction)\n",
    "        rews.append(reward)\n",
    "        \n",
    "    return obs, acts, rews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # hyper parameters\n",
    "    env = NavigationTask()\n",
    "    input_size = np.shape(env.getStateRep())\n",
    "    hparams = {\n",
    "            'input_size': input_size,\n",
    "            'num_actions': 10,\n",
    "            'learning_rate': 0.1,\n",
    "            'epiode_length': 6\n",
    "    }\n",
    "\n",
    "    # environment params\n",
    "    eparams = {\n",
    "            'num_batches': 10,\n",
    "            'ep_per_batch': 1000\n",
    "    }\n",
    "\n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "\n",
    "        pi = LSTMPolicy(hparams['input_size'], hparams['num_actions'])\n",
    "\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        for batch in range(0, eparams['num_batches']):\n",
    "            print('=====\\nBATCH {}\\n===='.format(batch))\n",
    "            num = 0\n",
    "            for i in range(0, eparams['ep_per_batch']):\n",
    "                obs, acts, rews = policyRollout(pi, hparams)\n",
    "                c, h = pi.get_initial_features()\n",
    "                num += 1 if 1 in rews else 0\n",
    "                pi.train_step(obs, acts, rews, c, h)\n",
    "            print(\"number of times reward\", num)\n",
    "            c, h = pi.get_initial_features()\n",
    "            obs, acts, rews = policyRollout(pi, hparams)\n",
    "            print(\"loss\",pi.train_step(obs, acts, rews, c, h))\n",
    "            print(\"Observation\", obs)\n",
    "            print(\"acts\", [np.argmax(a) for a in acts])\n",
    "            print(\"rews\", rews)\n",
    "        print(inference(pi, hparams))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs space (8,)\n",
      "x shape Tensor(\"x:0\", shape=(?, 8), dtype=float32)\n",
      "x shape Tensor(\"ExpandDims:0\", shape=(1, ?, 8), dtype=float32)\n",
      "step_size Tensor(\"strided_slice:0\", shape=(1,), dtype=int32)\n",
      "Tensor(\"rnn/transpose:0\", shape=(1, ?, 256), dtype=float32)\n",
      "x as output Tensor(\"Reshape:0\", shape=(?, 256), dtype=float32)\n",
      "x shape 256\n",
      "size 1\n",
      "x shape 256\n",
      "size 10\n",
      "logits Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n",
      "sample Tensor(\"strided_slice_3:0\", shape=(10,), dtype=float32)\n",
      "self.probs Tensor(\"strided_slice_4:0\", shape=(10,), dtype=float32)\n",
      "WARNING:tensorflow:From C:\\Users\\adity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "=====\n",
      "BATCH 0\n",
      "====\n",
      "number of times reward 628\n",
      "loss [None, 0.87906951, array([[-2.25473547, -3.30658746, -1.56206   , -4.9525013 , -4.45707035,\n",
      "        -3.21698928, -1.70864379, -2.36978531,  3.87012339,  1.50325024],\n",
      "       [-0.26923299,  0.09901207,  5.06077433, -2.16542625, -2.40653205,\n",
      "        -1.75463212, -0.89927763, -0.70125329, -0.3791348 , -0.81523824],\n",
      "       [-1.40546775, -4.36129522,  1.18376529, -7.52402067, -5.74223566,\n",
      "        -3.87255692, -0.36408207, -1.42256701,  3.83765936,  0.80662787],\n",
      "       [-0.14512995, -1.93742394,  0.29104522, -3.32011175, -2.70341086,\n",
      "        -1.68897784, -0.1212653 , -0.25262964,  1.24274135,  0.77211595],\n",
      "       [-0.27992263, -1.98292649,  0.32013243, -3.41280437, -2.74924612,\n",
      "        -1.85029233, -0.05564138, -0.33335656,  1.27270532,  0.74556494],\n",
      "       [-0.28474462, -2.08532   ,  0.26030165, -3.51483536, -2.84910798,\n",
      "        -1.91432905, -0.06541347, -0.35121697,  1.33677459,  0.81283307]], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  4.,  4.]), array([ 0.,  4.,  1.,  0.,  0.,  0.,  4.,  4.]), array([ 0.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.])]\n",
      "acts [8, 2, 8, 9, 7, 9]\n",
      "rews [0, 0, 1, 1, 1, 1]\n",
      "=====\n",
      "BATCH 1\n",
      "====\n",
      "number of times reward 976\n",
      "loss [None, 0.96260011, array([[-2.34050894, -3.35505581, -2.54970789, -4.81085396, -4.49215794,\n",
      "        -3.12493134, -2.36940908, -2.55954051,  3.71084595,  1.88966882],\n",
      "       [-1.45911539,  0.41707757,  6.81478596, -2.67609048, -2.32203627,\n",
      "        -1.69663203, -1.52464867, -0.87577492, -1.2425524 , -1.00639367],\n",
      "       [-0.28232762, -3.44224191, -0.88591713, -8.6429863 , -5.91218662,\n",
      "        -2.1891396 , -0.73505211, -0.34984753,  3.43736124,  1.66257739],\n",
      "       [ 0.26881728, -0.2720266 , -0.15884468, -2.17825866, -1.40261698,\n",
      "        -0.22528487,  0.0703997 ,  0.21985029,  0.08324399,  0.42956635],\n",
      "       [ 0.30136195, -0.05993594, -0.10220811, -1.3444314 , -0.6415686 ,\n",
      "        -0.02523604,  0.19346818,  0.24053237, -0.08751875,  0.20981669],\n",
      "       [ 0.21199009, -0.33226123, -0.43366811, -1.70914245, -0.89615268,\n",
      "        -0.16457959,  0.10122558,  0.08979429,  0.20548368,  0.35479307]], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  4.,  4.]), array([ 0.,  4.,  1.,  0.,  0.,  0.,  4.,  4.]), array([ 0.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.])]\n",
      "acts [8, 2, 8, 9, 9, 5]\n",
      "rews [0, 0, 1, 1, 1, 1]\n",
      "=====\n",
      "BATCH 2\n",
      "====\n",
      "number of times reward 952\n",
      "loss [None, 1.1511114, array([[-3.06956744, -3.29243398, -2.65192962, -3.78753257, -4.39110279,\n",
      "        -2.96745777, -2.6662097 , -2.52591562,  2.34460187,  2.76562834],\n",
      "       [-1.25321984, -0.21363947,  6.79042387, -1.90766931, -3.22623253,\n",
      "        -1.76131618, -2.25131631,  0.06348354, -1.81772637, -0.70656824],\n",
      "       [-1.37336349, -3.84158039, -0.70903349, -7.42480421, -6.74650526,\n",
      "        -2.50439239, -1.56294274,  1.88482904,  2.0292635 ,  2.55440545],\n",
      "       [ 0.38619009, -1.0240016 , -0.07237229, -2.21530771, -2.33453274,\n",
      "         0.25996721,  0.10194676,  0.6216675 ,  0.06051927,  0.35102382],\n",
      "       [ 0.39606732, -0.34792784,  0.17796345, -1.13731718, -1.25408781,\n",
      "         0.17353851,  0.13902888,  0.44651785, -0.2001234 ,  0.1098028 ],\n",
      "       [ 0.3939234 , -0.45769107,  0.12817906, -1.22070503, -1.33396709,\n",
      "         0.16538453,  0.11633965,  0.44989997, -0.11585072,  0.16393648]], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  4.,  4.]), array([ 0.,  4.,  1.,  0.,  0.,  0.,  4.,  4.]), array([ 0.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.])]\n",
      "acts [9, 2, 8, 0, 0, 2]\n",
      "rews [0, 0, 1, 1, 1, 1]\n",
      "=====\n",
      "BATCH 3\n",
      "====\n",
      "number of times reward 960\n",
      "loss [None, 1.6087731, array([[-3.33499813, -3.6449244 , -3.20382571, -4.30559778, -4.54919815,\n",
      "        -3.56243062, -2.86682987, -3.77929473,  3.17686534,  2.28703284],\n",
      "       [-2.33998322, -0.72111595,  7.6241293 , -2.00157213, -3.57819414,\n",
      "        -2.57192039, -1.94640267, -0.45035988, -1.41848254, -1.22038662],\n",
      "       [-2.24094844, -4.22202682, -1.35277128, -7.73381758, -7.24959898,\n",
      "        -2.97639036,  0.18998688, -0.40424815,  4.18565798,  1.47603238],\n",
      "       [ 0.22370785, -0.23258059,  0.10202324, -1.90765977, -2.13670325,\n",
      "         0.02120456,  0.3731336 ,  0.35942954,  0.32734644, -0.12020748],\n",
      "       [ 0.17367415, -0.14401001,  0.08868329, -1.28516102, -1.46087587,\n",
      "        -0.13351446,  0.34392661,  0.40449208,  0.17206043, -0.07885537],\n",
      "       [ 0.15527911, -0.24178492,  0.01366436, -1.34968817, -1.49715137,\n",
      "        -0.13066736,  0.35578188,  0.40587193,  0.27814257, -0.04115362]], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  4.,  4.]), array([ 0.,  4.,  1.,  0.,  0.,  0.,  4.,  4.]), array([ 0.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.])]\n",
      "acts [9, 2, 9, 9, 0, 6]\n",
      "rews [0, 0, 1, 1, 1, 1]\n",
      "=====\n",
      "BATCH 4\n",
      "====\n",
      "number of times reward 988\n",
      "loss [None, 1.1213099, array([[ -3.86438632e+00,  -3.52607369e+00,  -3.54922080e+00,\n",
      "         -3.93900490e+00,  -4.01256418e+00,  -3.56104827e+00,\n",
      "         -3.37756896e+00,  -3.88792515e+00,   2.23317552e+00,\n",
      "          2.60251641e+00],\n",
      "       [ -3.38446045e+00,  -9.03773367e-01,   7.73598766e+00,\n",
      "         -2.36951303e+00,  -4.10953712e+00,  -2.64711833e+00,\n",
      "         -2.01105690e+00,  -6.63332164e-01,  -1.56309700e+00,\n",
      "         -9.22873139e-01],\n",
      "       [ -2.96110201e+00,  -4.43955708e+00,  -2.16556692e+00,\n",
      "         -5.75885725e+00,  -7.34358406e+00,  -3.32316279e+00,\n",
      "          7.77574956e-01,  -8.33131075e-01,   2.43746257e+00,\n",
      "          3.45169449e+00],\n",
      "       [ -8.03941041e-02,  -5.56471765e-01,   1.93827763e-01,\n",
      "         -2.16274905e+00,  -2.69217539e+00,   1.60815939e-01,\n",
      "          4.17379469e-01,   6.19448423e-01,  -1.86435863e-01,\n",
      "          6.01800025e-01],\n",
      "       [  1.37349740e-02,   5.24129495e-02,   8.98832977e-02,\n",
      "         -4.16818142e-01,  -7.24525452e-01,   3.47451940e-02,\n",
      "          1.38165861e-01,   4.53955293e-01,  -2.68096417e-01,\n",
      "          1.71329856e-01],\n",
      "       [  9.25924852e-02,   8.18095356e-02,  -8.25241059e-02,\n",
      "          3.78679037e-02,  -1.49944827e-01,   7.86384642e-02,\n",
      "          1.80359930e-05,   2.83827096e-01,  -8.92968625e-02,\n",
      "          8.91105309e-02]], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  4.,  4.]), array([ 0.,  4.,  1.,  0.,  0.,  0.,  4.,  4.]), array([ 0.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  1.,  0.,  0.,  0.,  4.,  4.])]\n",
      "acts [9, 2, 9, 6, 1, 4]\n",
      "rews [0, 0, 1, 1, 1, 1]\n",
      "=====\n",
      "BATCH 5\n",
      "====\n",
      "number of times reward 995\n",
      "loss [None, 1.1792569, array([[-4.53275824, -3.79041886, -4.21249199, -4.06374741, -4.92020273,\n",
      "        -3.64380956, -3.33299398, -4.62327719,  1.84063029,  3.2857573 ],\n",
      "       [-3.71060944, -1.66489387,  8.05467033, -2.74717522, -4.37836361,\n",
      "        -3.27020526, -2.66530252, -0.84578133, -1.45878184, -0.63040328],\n",
      "       [-4.61461639, -5.26546001, -3.42730141, -6.24002552, -8.97061157,\n",
      "        -4.28546906, -0.82231563, -2.72886729,  1.81041825,  6.17742252],\n",
      "       [-0.52894777, -0.66454893,  0.40433654, -2.31462646, -2.41552591,\n",
      "         0.11246432,  0.41459635,  0.18266958,  0.1125305 ,  0.81140727],\n",
      "       [-0.14019501,  0.05682369,  0.43585005, -0.67658621, -0.72064787,\n",
      "         0.06772871,  0.09938595,  0.4987345 , -0.1744259 ,  0.10299196],\n",
      "       [ 0.40267029,  0.50561321,  1.10903847,  0.09918007,  0.25595444,\n",
      "         0.31701112,  0.16823637,  1.01539373, -0.44278061, -0.43476143]], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  4.,  4.]), array([ 0.,  4.,  1.,  0.,  0.,  0.,  4.,  4.]), array([ 0.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  0.,  0.,  1.,  4.,  4.])]\n",
      "acts [9, 2, 9, 5, 4, 0]\n",
      "rews [0, 0, 1, 1, 1, 1]\n",
      "=====\n",
      "BATCH 6\n",
      "====\n",
      "number of times reward 992\n",
      "loss [None, 1.2839839, array([[ -4.34923172e+00,  -4.40473223e+00,  -4.33890820e+00,\n",
      "         -5.16665363e+00,  -4.70192194e+00,  -4.03773737e+00,\n",
      "         -4.02024746e+00,  -4.36246967e+00,   3.34019232e+00,\n",
      "          1.74125183e+00],\n",
      "       [ -4.06322765e+00,  -1.51219893e+00,   7.27967167e+00,\n",
      "         -1.05885887e+00,  -4.28489780e+00,  -4.57601118e+00,\n",
      "         -1.61090553e+00,  -1.95831990e+00,  -1.06873226e+00,\n",
      "         -8.09220254e-01],\n",
      "       [ -3.93945622e+00,  -3.82418752e+00,  -2.51575875e+00,\n",
      "         -5.08247805e+00,  -5.90120935e+00,  -3.54035425e+00,\n",
      "         -1.02633822e+00,  -1.42616570e+00,   2.40716434e+00,\n",
      "          3.28953457e+00],\n",
      "       [ -1.22930503e+00,  -4.56529975e-01,  -1.43945351e-01,\n",
      "         -2.25583243e+00,  -2.82390571e+00,   5.64191490e-02,\n",
      "          2.23505065e-01,  -2.67928660e-01,   7.01573014e-01,\n",
      "          9.64133561e-01],\n",
      "       [ -3.23023438e-01,   3.04705262e-01,   4.42425273e-02,\n",
      "          1.07625365e-01,  -3.49817932e-01,  -2.40821317e-01,\n",
      "          6.99835345e-02,   1.78710092e-03,  -3.37853134e-02,\n",
      "          4.54631187e-02],\n",
      "       [ -4.37874138e-01,   1.06292188e-01,   4.37143534e-01,\n",
      "          7.77352512e-01,  -1.23533756e-02,  -8.01995933e-01,\n",
      "         -7.43799731e-02,  -3.84396166e-01,  -1.23053685e-01,\n",
      "         -2.05418319e-01]], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  4.,  4.]), array([ 0.,  4.,  1.,  0.,  0.,  0.,  4.,  4.]), array([ 0.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  1.,  0.,  0.,  0.,  4.,  4.])]\n",
      "acts [9, 2, 8, 8, 1, 2]\n",
      "rews [0, 0, 1, 1, 1, 1]\n",
      "=====\n",
      "BATCH 7\n",
      "====\n",
      "number of times reward 985\n",
      "loss [None, 1.0562164, array([[-4.32614946, -4.52447128, -3.99486256, -5.32982159, -5.14940023,\n",
      "        -4.40778065, -3.96049666, -3.99114609,  2.26900339,  2.28574753],\n",
      "       [-3.60530996, -1.94091475,  7.16668129, -2.40354347, -4.47877741,\n",
      "        -4.24989891, -1.9823041 , -1.27083278, -0.81572658, -0.91837943],\n",
      "       [-3.63189983, -3.59142923, -1.71143126, -4.89873981, -4.67725706,\n",
      "        -3.14404202, -0.71364057, -0.90590841,  1.88544238,  1.57730532],\n",
      "       [-0.86055279, -0.65016353, -0.04376081, -2.54361844, -2.1541481 ,\n",
      "         0.01717706,  0.06491605, -0.01157334,  0.61349779,  0.3200922 ],\n",
      "       [-0.39915648, -0.21488743,  0.01093252, -0.54081333, -0.61045635,\n",
      "        -0.30596966, -0.06114405,  0.03138131,  0.16153105,  0.07590824],\n",
      "       [-0.35028228, -0.383984  ,  0.13584591, -0.10342965, -0.35634384,\n",
      "        -0.63464659, -0.20610857, -0.14943139,  0.11673279,  0.08244587]], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  4.,  4.]), array([ 0.,  4.,  1.,  0.,  0.,  0.,  4.,  4.]), array([ 0.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.])]\n",
      "acts [9, 2, 9, 8, 8, 8]\n",
      "rews [0, 0, 1, 1, 1, 1]\n",
      "=====\n",
      "BATCH 8\n",
      "====\n",
      "number of times reward 981\n",
      "loss [None, 1.120414, array([[-3.71643853, -4.33439541, -4.65453148, -4.97154474, -4.70492887,\n",
      "        -4.07839441, -4.09506655, -3.97198153,  2.20756221,  2.19083095],\n",
      "       [-2.67096472, -1.24873114,  7.4785037 , -2.29033041, -3.42906666,\n",
      "        -3.47283673, -1.98443806, -0.92667544, -1.18414617, -0.78795916],\n",
      "       [-2.51832509, -4.01702738, -2.17693496, -4.80136633, -4.74187136,\n",
      "        -2.4552474 , -1.70202577, -1.18865693,  1.40606189,  1.93516529],\n",
      "       [-0.05185642, -0.40002596, -0.24235964, -1.8250159 , -1.68145716,\n",
      "        -0.18412851, -0.13450815,  0.01558331,  0.26874751,  0.37474045],\n",
      "       [-0.11263285, -0.32265803, -0.39333469, -0.63060325, -0.63629138,\n",
      "        -0.34087816, -0.18989702, -0.13770029,  0.20717126,  0.2692771 ],\n",
      "       [-0.09135053, -0.3770726 , -0.45377555, -0.27363223, -0.3707937 ,\n",
      "        -0.44744787, -0.23810628, -0.27062136,  0.21374696,  0.25741342]], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  4.,  4.]), array([ 0.,  4.,  1.,  0.,  0.,  0.,  4.,  4.]), array([ 0.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.])]\n",
      "acts [8, 2, 9, 0, 6, 9]\n",
      "rews [0, 0, 1, 1, 1, 1]\n",
      "=====\n",
      "BATCH 9\n",
      "====\n",
      "number of times reward 991\n",
      "loss [None, 1.2180284, array([[ -4.06407356e+00,  -4.65264988e+00,  -4.84941292e+00,\n",
      "         -5.13688898e+00,  -4.84631920e+00,  -4.48527002e+00,\n",
      "         -4.16291189e+00,  -4.35350037e+00,   1.95050848e+00,\n",
      "          2.38963008e+00],\n",
      "       [ -2.87175226e+00,  -1.11680424e+00,   7.96612835e+00,\n",
      "         -2.12204719e+00,  -3.33420038e+00,  -3.23534465e+00,\n",
      "         -1.74423718e+00,  -1.02844870e+00,  -9.89223838e-01,\n",
      "         -1.09344399e+00],\n",
      "       [ -2.97617793e+00,  -4.53002501e+00,  -2.81476569e+00,\n",
      "         -5.75731516e+00,  -5.68217278e+00,  -3.11444163e+00,\n",
      "         -1.62865031e+00,  -1.52302730e+00,   1.97451758e+00,\n",
      "          1.93651736e+00],\n",
      "       [ -4.91277993e-01,  -4.95955884e-01,  -3.83958966e-01,\n",
      "         -2.73592162e+00,  -2.59085560e+00,  -5.78940241e-03,\n",
      "          1.82577789e-01,  -2.95077354e-01,   6.78476572e-01,\n",
      "          4.91907835e-01],\n",
      "       [ -2.34432831e-01,  -1.65022343e-01,  -2.68800735e-01,\n",
      "         -7.63470590e-01,  -7.82684505e-01,  -1.67893916e-01,\n",
      "         -1.74348038e-02,  -1.08598784e-01,   2.53920078e-01,\n",
      "          1.78489983e-01],\n",
      "       [ -9.93535817e-02,  -1.12932421e-01,  -1.60138056e-01,\n",
      "         -3.38469565e-01,  -4.50329483e-01,  -2.18904451e-01,\n",
      "         -6.40739724e-02,  -1.17764741e-01,   1.50577009e-01,\n",
      "          1.21815123e-01]], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  4.,  4.]), array([ 0.,  4.,  1.,  0.,  0.,  0.,  4.,  4.]), array([ 0.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.])]\n",
      "acts [8, 2, 8, 5, 9, 2]\n",
      "rews [0, 0, 1, 1, 1, 1]\n",
      "([array([ 0.,  0.,  1.,  0.,  0.,  0.,  4.,  4.]), array([ 0.,  4.,  1.,  0.,  0.,  0.,  4.,  4.]), array([ 0.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.]), array([ 4.,  4.,  0.,  1.,  0.,  0.,  4.,  4.])], [9, 2, 8, 8, 8, 8], [0, 0, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
