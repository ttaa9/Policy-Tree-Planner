{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np, numpy.random as npr, random as r\n",
    "import tensorflow as tf  \n",
    "from NavTask import NavigationTask\n",
    "import tensorflow.contrib.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(x, size, name, initializer=None, bias_init=0):\n",
    "    print(\"x shape\",x.get_shape()[1])\n",
    "    print(\"size\", size)\n",
    "    w = tf.get_variable(name + \"/w\", [x.get_shape()[1], size], initializer=initializer)\n",
    "    b = tf.get_variable(name + \"/b\", [size], initializer=tf.constant_initializer(bias_init))\n",
    "    return tf.matmul(x, w) + b\n",
    "\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer\n",
    "\n",
    "def categorical_sample(logits, d):\n",
    "    value = tf.squeeze(tf.multinomial(logits - tf.reduce_max(logits, [1], keep_dims=True), 1), [1])\n",
    "    return tf.one_hot(value, d)\n",
    "\n",
    "class LSTMPolicy(object):\n",
    "\n",
    "    def __init__(self, ob_space, ac_space):\n",
    "        \n",
    "        print(\"obs space\", ob_space)\n",
    "        # x is the observations/states for the length of the episode\n",
    "        self.x = x = tf.placeholder(tf.float32,[None] + list(ob_space), name=\"x\")\n",
    "        print(\"x shape\", x)\n",
    "        size = 256\n",
    "        \n",
    "        # introduce a \"fake\" batch dimension of 1 to do LSTM over time dim\n",
    "        x = tf.expand_dims(x, [0])\n",
    "        \n",
    "        print(\"x shape\", x)\n",
    "        lstm = rnn.BasicLSTMCell(size, state_is_tuple=True)\n",
    "        self.state_size = lstm.state_size\n",
    "\n",
    "        #Step size for truncated backprop using the ob_space, basically [batch_size]\n",
    "        self.step_size = step_size = tf.shape(self.x)[:1]\n",
    "        print(\"step_size\", step_size)\n",
    "        \n",
    "        # defining the cell state and output state of the LSTM\n",
    "        c_init = np.zeros((1, lstm.state_size.c), np.float32)\n",
    "        h_init = np.zeros((1, lstm.state_size.h), np.float32)\n",
    "        self.state_init = [c_init, h_init]\n",
    "        \n",
    "        #defining placeholders so that we can input during training and inference, Example: during rollout you want to input these values \n",
    "        c_in = tf.placeholder(tf.float32, [1, lstm.state_size.c], name='c_in')\n",
    "        h_in = tf.placeholder(tf.float32, [1, lstm.state_size.h], name='h_in')\n",
    "        self.state_in = [c_in, h_in]\n",
    "        \n",
    "        state_in = rnn.LSTMStateTuple(c_in, h_in)\n",
    "        \n",
    "        lstm_outputs, lstm_state = tf.nn.dynamic_rnn(\n",
    "            lstm, x, initial_state=state_in, sequence_length=step_size,\n",
    "            time_major=False)\n",
    "        lstm_c, lstm_h = lstm_state\n",
    "        \n",
    "        print(lstm_outputs)\n",
    "        x = tf.reshape(lstm_outputs, [-1, size])\n",
    "        print(\"x as output\", x)\n",
    "        \n",
    "        # vf == value-function?? is one-dimenstion, so basically value for the given state? \n",
    "        self.vf = tf.reshape(linear(x, 1, \"value\", normalized_columns_initializer(1.0)), [-1])\n",
    "        \n",
    "        # can be used to later to get the values \n",
    "        self.state_out = [lstm_c[:1, :], lstm_h[:1, :]]\n",
    "\n",
    "        # [0, :] means pick action of first state from batch. Hardcoded b/c\n",
    "        # batch=1 during rollout collection. Its not used during batch training.\n",
    "        \n",
    "        self.logits = linear(x, ac_space, \"action\", normalized_columns_initializer(0.01))\n",
    "        print(\"logits\", self.logits)\n",
    "        self.sample = categorical_sample(self.logits, ac_space)[0, :]\n",
    "        print(\"sample\", self.sample)\n",
    "        self.probs = tf.nn.softmax(self.logits, dim=-1)[0, :]\n",
    "        print(\"self.probs\", self.probs)\n",
    "        \n",
    "     \n",
    "        self.log_prob = log_prob = tf.nn.log_softmax(self.logits,  dim=-1)\n",
    "        self.prob_tf = tf.nn.softmax(self.logits)\n",
    "        \n",
    "        # training part of graph\n",
    "        self.ac = tf.placeholder(tf.float32, [None, ac_space], name=\"ac\")\n",
    "        self.adv = tf.placeholder(tf.float32, [None], name=\"adv\")\n",
    "        self.cumaltiveReward = tf.reduce_sum(self.adv)\n",
    "        \n",
    "  \n",
    "        self.cross_entropy_loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.log_prob, labels=self.ac)\n",
    "        self.loss =  tf.reduce_mean(self.cross_entropy_loss)\n",
    "        self.gradients = tf.train.AdamOptimizer(0.01).compute_gradients(self.loss)\n",
    "        for i, (grad, var) in enumerate(self.gradients):\n",
    "            if grad is not None:\n",
    "                self.gradients[i] = (grad * self.cumaltiveReward, var)\n",
    "                \n",
    "        self._train = tf.train.AdamOptimizer(0.01).apply_gradients(self.gradients)\n",
    "       \n",
    "    \n",
    "    def get_initial_features(self):\n",
    "        # Call this function to get reseted lstm memory cells\n",
    "        return self.state_init\n",
    "\n",
    "    def act(self, ob, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run([self.sample, self.vf] + self.state_out,\n",
    "                        {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})\n",
    "        return sess.run(self.pred, {self.input:x})\n",
    "\n",
    "    def act_inference(self, ob, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run([self.probs, self.sample, self.vf] + self.state_out,\n",
    "                        {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})\n",
    "\n",
    "    def value(self, ob, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run(self.vf, {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})[0]\n",
    "\n",
    "    def train_step(self, obs, acts, advantages, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        batch_feed = { self.x: obs, self.ac: acts, self.adv: advantages, self.state_in[0]: c, self.state_in[1]: h}\n",
    "        return sess.run([self._train, self.loss, self.cross_entropy_loss], feed_dict=batch_feed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(agent, hparams):\n",
    "    #\"Runs one episode\"\n",
    "    episode_length = hparams['epiode_length']\n",
    "    env = NavigationTask(3,3)\n",
    "    obs, acts, rews = [], [], []\n",
    "    c, h = agent.get_initial_features()\n",
    "    for i in range(0, episode_length): \n",
    "        state = env.getStateRep()\n",
    "        obs.append(state)\n",
    "        \n",
    "        actionProb, sampleAction , _ , c, h  = agent.act_inference(state, c, h)\n",
    "        \n",
    "        action = actionProb.argmax()\n",
    "        sampleActionIndex = sampleAction.argmax()\n",
    "        \n",
    "        env.performAction(action)\n",
    "        newState  = env.getStateRep()\n",
    "        reward = env.getReward(distance_based=True) \n",
    "        \n",
    "        acts.append(action)\n",
    "        rews.append(reward)\n",
    "        \n",
    "    return obs, acts, rews  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policyRollout(agent, hparams):\n",
    "    \n",
    "    #\"Runs one episode\"\n",
    "    episode_length = hparams['epiode_length']\n",
    "    env = NavigationTask(3,3)\n",
    "    obs, acts, rews = [], [], []\n",
    "    c, h = agent.get_initial_features()\n",
    "    \n",
    "    for i in range(0, episode_length): \n",
    "        \n",
    "        state = env.getStateRep()\n",
    "        obs.append(state)\n",
    "        actionProb, sampleAction , _ , c, h  = agent.act_inference(state, c, h)\n",
    "      \n",
    "        action = actionProb.argmax()\n",
    "        sampleActionIndex = sampleAction.argmax()\n",
    "        \n",
    "        env.performAction(sampleActionIndex)\n",
    "        newState  = env.getStateRep()\n",
    "        reward = env.getReward(distance_based=True) \n",
    "        \n",
    "        acts.append(sampleAction)\n",
    "        rews.append(reward)\n",
    "        \n",
    "    return obs, acts, rews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # hyper parameters\n",
    "    env = NavigationTask()\n",
    "    input_size = np.shape(env.getStateRep())\n",
    "    hparams = {\n",
    "            'input_size': input_size,\n",
    "            'num_actions': 10,\n",
    "            'learning_rate': 0.1,\n",
    "            'epiode_length': 6\n",
    "    }\n",
    "\n",
    "    # environment params\n",
    "    eparams = {\n",
    "            'num_batches': 10,\n",
    "            'ep_per_batch': 500\n",
    "    }\n",
    "\n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "\n",
    "        pi = LSTMPolicy(hparams['input_size'], hparams['num_actions'])\n",
    "\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        for batch in range(0, eparams['num_batches']):\n",
    "            print('=====\\nBATCH {}\\n===='.format(batch))\n",
    "            num = 0\n",
    "            for i in range(0, eparams['ep_per_batch']):\n",
    "                obs, acts, rews = policyRollout(pi, hparams)\n",
    "                c, h = pi.get_initial_features()\n",
    "                num += 1 if 1 in rews else 0\n",
    "                pi.train_step(obs, acts, rews, c, h)\n",
    "            print(\"number of times reward\", num)\n",
    "            c, h = pi.get_initial_features()\n",
    "            obs, acts, rews = policyRollout(pi, hparams)\n",
    "            print(\"loss\",pi.train_step(obs, acts, rews, c, h))\n",
    "            print(\"Observation\", obs)\n",
    "            print(\"acts\", [np.argmax(a) for a in acts])\n",
    "            print(\"rews\", rews)\n",
    "        print(inference(pi, hparams))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs space (8,)\n",
      "x shape Tensor(\"x:0\", shape=(?, 8), dtype=float32)\n",
      "x shape Tensor(\"ExpandDims:0\", shape=(1, ?, 8), dtype=float32)\n",
      "step_size Tensor(\"strided_slice:0\", shape=(1,), dtype=int32)\n",
      "Tensor(\"rnn/transpose:0\", shape=(1, ?, 256), dtype=float32)\n",
      "x as output Tensor(\"Reshape:0\", shape=(?, 256), dtype=float32)\n",
      "x shape 256\n",
      "size 1\n",
      "x shape 256\n",
      "size 10\n",
      "logits Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n",
      "sample Tensor(\"strided_slice_3:0\", shape=(10,), dtype=float32)\n",
      "self.probs Tensor(\"strided_slice_4:0\", shape=(10,), dtype=float32)\n",
      "WARNING:tensorflow:From C:\\Users\\adity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "=====\n",
      "BATCH 0\n",
      "====\n",
      "number of times reward 0\n",
      "loss [None, 1.205977e-05, array([  5.03050542e-05,   1.75236128e-05,   1.78813775e-06,\n",
      "         9.53673862e-07,   9.53673862e-07,   8.34464686e-07], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.])]\n",
      "acts [9, 4, 4, 4, 4, 4]\n",
      "rews [0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331]\n",
      "=====\n",
      "BATCH 1\n",
      "====\n",
      "number of times reward 0\n",
      "loss [None, 6.059729e-06, array([  2.96826729e-05,   5.84123791e-06,   3.57627812e-07,\n",
      "         2.38418551e-07,   1.19209282e-07,   1.19209282e-07], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.])]\n",
      "acts [9, 4, 4, 4, 4, 4]\n",
      "rews [0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331]\n",
      "=====\n",
      "BATCH 2\n",
      "====\n",
      "number of times reward 0\n",
      "loss [None, 3.5365154e-06, array([  1.77620259e-05,   2.86101886e-06,   2.38418551e-07,\n",
      "         1.19209282e-07,   1.19209282e-07,   1.19209282e-07], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.])]\n",
      "acts [9, 4, 4, 4, 4, 4]\n",
      "rews [0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331]\n",
      "=====\n",
      "BATCH 3\n",
      "====\n",
      "number of times reward 0\n",
      "loss [None, 2.2053612e-06, array([  1.10864021e-05,   1.90734681e-06,   1.19209282e-07,\n",
      "         1.19209282e-07,   0.00000000e+00,   0.00000000e+00], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.])]\n",
      "acts [9, 4, 4, 4, 4, 4]\n",
      "rews [0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331]\n",
      "=====\n",
      "BATCH 4\n",
      "====\n",
      "number of times reward 0\n",
      "loss [None, 1.4503752e-06, array([  7.27174029e-06,   1.31130128e-06,   1.19209282e-07,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.])]\n",
      "acts [9, 4, 4, 4, 4, 4]\n",
      "rews [0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331]\n",
      "=====\n",
      "BATCH 5\n",
      "====\n",
      "number of times reward 0\n",
      "loss [None, 1.0132769e-06, array([  4.88756905e-06,   1.07288304e-06,   1.19209282e-07,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.])]\n",
      "acts [9, 4, 4, 4, 4, 4]\n",
      "rews [0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331]\n",
      "=====\n",
      "BATCH 6\n",
      "====\n",
      "number of times reward 0\n",
      "loss [None, 7.1525477e-07, array([  3.33785465e-06,   9.53673862e-07,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.])]\n",
      "acts [9, 4, 4, 4, 4, 4]\n",
      "rews [0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331]\n",
      "=====\n",
      "BATCH 7\n",
      "====\n",
      "number of times reward 0\n",
      "loss [None, 5.165731e-07, array([  2.38418306e-06,   7.15255510e-07,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.])]\n",
      "acts [9, 4, 4, 4, 4, 4]\n",
      "rews [0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331]\n",
      "=====\n",
      "BATCH 8\n",
      "====\n",
      "number of times reward 0\n",
      "loss [None, 3.5762764e-07, array([  1.66892869e-06,   4.76837045e-07,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.])]\n",
      "acts [9, 4, 4, 4, 4, 4]\n",
      "rews [0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331]\n",
      "=====\n",
      "BATCH 9\n",
      "====\n",
      "number of times reward 0\n",
      "loss [None, 2.7815489e-07, array([  1.19209221e-06,   4.76837045e-07,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.])]\n",
      "acts [9, 4, 4, 4, 4, 4]\n",
      "rews [0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331]\n",
      "([array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  0.,  1.,  2.,  2.])], [9, 4, 4, 4, 4, 4], [0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331])\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
