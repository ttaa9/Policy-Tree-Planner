{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np, numpy.random as npr, random as r\n",
    "import tensorflow as tf  \n",
    "from NavTask import NavigationTask\n",
    "import tensorflow.contrib.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(x, size, name, initializer=None, bias_init=0):\n",
    "    print(\"x shape\",x.get_shape()[1])\n",
    "    print(\"size\", size)\n",
    "    w = tf.get_variable(name + \"/w\", [x.get_shape()[1], size], initializer=initializer)\n",
    "    b = tf.get_variable(name + \"/b\", [size], initializer=tf.constant_initializer(bias_init))\n",
    "    return tf.matmul(x, w) + b\n",
    "\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer\n",
    "\n",
    "def categorical_sample(logits, d):\n",
    "    value = tf.squeeze(tf.multinomial(logits - tf.reduce_max(logits, [1], keep_dims=True), 1), [1])\n",
    "    return tf.one_hot(value, d)\n",
    "\n",
    "class LSTMPolicy(object):\n",
    "\n",
    "    def __init__(self, ob_space, ac_space):\n",
    "        \n",
    "        print(\"obs space\", ob_space)\n",
    "        # x is the observations/states for the length of the episode\n",
    "        self.x = x = tf.placeholder(tf.float32,[None] + list(ob_space), name=\"x\")\n",
    "        print(\"x shape\", x)\n",
    "        size = 256\n",
    "        \n",
    "        # introduce a \"fake\" batch dimension of 1 to do LSTM over time dim\n",
    "        x = tf.expand_dims(x, [0])\n",
    "        \n",
    "        print(\"x shape\", x)\n",
    "        lstm = rnn.BasicLSTMCell(size, state_is_tuple=True)\n",
    "        self.state_size = lstm.state_size\n",
    "\n",
    "        #Step size for truncated backprop using the ob_space, basically [batch_size]\n",
    "        self.step_size = step_size = tf.shape(self.x)[:1]\n",
    "        print(\"step_size\", step_size)\n",
    "        \n",
    "        # defining the cell state and output state of the LSTM\n",
    "        c_init = np.zeros((1, lstm.state_size.c), np.float32)\n",
    "        h_init = np.zeros((1, lstm.state_size.h), np.float32)\n",
    "        self.state_init = [c_init, h_init]\n",
    "        \n",
    "        #defining placeholders so that we can input during training and inference, Example: during rollout you want to input these values \n",
    "        c_in = tf.placeholder(tf.float32, [1, lstm.state_size.c], name='c_in')\n",
    "        h_in = tf.placeholder(tf.float32, [1, lstm.state_size.h], name='h_in')\n",
    "        self.state_in = [c_in, h_in]\n",
    "        \n",
    "        state_in = rnn.LSTMStateTuple(c_in, h_in)\n",
    "        \n",
    "        lstm_outputs, lstm_state = tf.nn.dynamic_rnn(\n",
    "            lstm, x, initial_state=state_in, sequence_length=step_size,\n",
    "            time_major=False)\n",
    "        lstm_c, lstm_h = lstm_state\n",
    "        \n",
    "        print(lstm_outputs)\n",
    "        x = tf.reshape(lstm_outputs, [-1, size])\n",
    "        print(\"x as output\", x)\n",
    "        \n",
    "        # vf == value-function?? is one-dimenstion, so basically value for the given state? \n",
    "        self.vf = tf.reshape(linear(x, 1, \"value\", normalized_columns_initializer(1.0)), [-1])\n",
    "        \n",
    "        # can be used to later to get the values \n",
    "        self.state_out = [lstm_c[:1, :], lstm_h[:1, :]]\n",
    "\n",
    "        # [0, :] means pick action of first state from batch. Hardcoded b/c\n",
    "        # batch=1 during rollout collection. Its not used during batch training.\n",
    "        \n",
    "        self.logits = linear(x, ac_space, \"action\", normalized_columns_initializer(0.01))\n",
    "        print(\"logits\", self.logits)\n",
    "        self.sample = categorical_sample(self.logits, ac_space)[0, :]\n",
    "        print(\"sample\", self.sample)\n",
    "        self.probs = tf.nn.softmax(self.logits, dim=-1)[0, :]\n",
    "        print(\"self.probs\", self.probs)\n",
    "        \n",
    "     \n",
    "        self.log_prob = log_prob = tf.nn.log_softmax(self.logits,  dim=-1)\n",
    "        self.prob_tf = tf.nn.softmax(self.logits)\n",
    "        \n",
    "        # training part of graph\n",
    "        self.ac = tf.placeholder(tf.float32, [None, ac_space], name=\"ac\")\n",
    "        self.adv = tf.placeholder(tf.float32, [None], name=\"adv\")\n",
    "        self.cumaltiveReward = tf.reduce_sum(self.adv)\n",
    "        self.entropy =  tf.reduce_mean(tf.reduce_sum(self.prob_tf  * log_prob, 1))\n",
    "  \n",
    "        self.cross_entropy_loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.log_prob, labels=self.ac)\n",
    "        self.loss =  tf.reduce_mean(self.cross_entropy_loss) + 0.1*self.entropy \n",
    "        self.gradients = tf.train.AdamOptimizer(0.001).compute_gradients(self.loss)\n",
    "        for i, (grad, var) in enumerate(self.gradients):\n",
    "            if grad is not None:\n",
    "                self.gradients[i] = (grad * self.cumaltiveReward, var)\n",
    "                \n",
    "        self._train = tf.train.AdamOptimizer(0.001).apply_gradients(self.gradients)\n",
    "       \n",
    "    \n",
    "    def get_initial_features(self):\n",
    "        # Call this function to get reseted lstm memory cells\n",
    "        return self.state_init\n",
    "\n",
    "    def act(self, ob, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run([self.sample, self.vf] + self.state_out,\n",
    "                        {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})\n",
    "        return sess.run(self.pred, {self.input:x})\n",
    "\n",
    "    def act_inference(self, ob, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run([self.probs, self.sample, self.vf] + self.state_out,\n",
    "                        {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})\n",
    "\n",
    "    def value(self, ob, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run(self.vf, {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})[0]\n",
    "\n",
    "    def train_step(self, obs, acts, advantages, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        batch_feed = { self.x: obs, self.ac: acts, self.adv: advantages, self.state_in[0]: c, self.state_in[1]: h}\n",
    "        return sess.run([self._train, self.loss, self.logits], feed_dict=batch_feed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(agent, hparams):\n",
    "    #\"Runs one episode\"\n",
    "    episode_length = hparams['epiode_length']\n",
    "    env = NavigationTask(3,3)\n",
    "    obs, acts, rews = [], [], []\n",
    "    c, h = agent.get_initial_features()\n",
    "    for i in range(0, episode_length): \n",
    "        state = env.getStateRep()\n",
    "        obs.append(state)\n",
    "        \n",
    "        actionProb, sampleAction , _ , c, h  = agent.act_inference(state, c, h)\n",
    "        \n",
    "        action = actionProb.argmax()\n",
    "        sampleActionIndex = sampleAction.argmax()\n",
    "        \n",
    "        env.performAction(action)\n",
    "        newState  = env.getStateRep()\n",
    "        reward = env.getReward(distance_based=True) \n",
    "        \n",
    "        acts.append(action)\n",
    "        rews.append(reward)\n",
    "        \n",
    "    return obs, acts, rews  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policyRollout(agent, hparams):\n",
    "    \n",
    "    #\"Runs one episode\"\n",
    "    episode_length = hparams['epiode_length']\n",
    "    env = NavigationTask(3,3)\n",
    "    obs, acts, rews = [], [], []\n",
    "    c, h = agent.get_initial_features()\n",
    "    \n",
    "    for i in range(0, episode_length): \n",
    "        \n",
    "        state = env.getStateRep()\n",
    "        obs.append(state)\n",
    "        actionProb, sampleAction , _ , c, h  = agent.act_inference(state, c, h)\n",
    "      \n",
    "        action = actionProb.argmax()\n",
    "        sampleActionIndex = sampleAction.argmax()\n",
    "        \n",
    "        env.performAction(sampleActionIndex)\n",
    "        newState  = env.getStateRep()\n",
    "        reward = env.getReward(distance_based=True) \n",
    "        \n",
    "        acts.append(sampleAction)\n",
    "        rews.append(reward)\n",
    "        \n",
    "    return obs, acts, rews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # hyper parameters\n",
    "    env = NavigationTask()\n",
    "    input_size = np.shape(env.getStateRep())\n",
    "    hparams = {\n",
    "            'input_size': input_size,\n",
    "            'num_actions': 10,\n",
    "            'learning_rate': 0.1,\n",
    "            'epiode_length': 6\n",
    "    }\n",
    "\n",
    "    # environment params\n",
    "    eparams = {\n",
    "            'num_batches': 10,\n",
    "            'ep_per_batch': 1000\n",
    "    }\n",
    "\n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "\n",
    "        pi = LSTMPolicy(hparams['input_size'], hparams['num_actions'])\n",
    "\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        for batch in range(0, eparams['num_batches']):\n",
    "            print('=====\\nBATCH {}\\n===='.format(batch))\n",
    "            num = 0\n",
    "            for i in range(0, eparams['ep_per_batch']):\n",
    "                obs, acts, rews = policyRollout(pi, hparams)\n",
    "                c, h = pi.get_initial_features()\n",
    "                num += 1 if 1 in rews else 0\n",
    "                pi.train_step(obs, acts, rews, c, h)\n",
    "            print(\"number of times reward\", num)\n",
    "            c, h = pi.get_initial_features()\n",
    "            obs, acts, rews = policyRollout(pi, hparams)\n",
    "            print(\"loss\",pi.train_step(obs, acts, rews, c, h))\n",
    "            print(\"Observation\", obs)\n",
    "            print(\"acts\", [np.argmax(a) for a in acts])\n",
    "            print(\"rews\", rews)\n",
    "        print(inference(pi, hparams))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs space (8,)\n",
      "x shape Tensor(\"x:0\", shape=(?, 8), dtype=float32)\n",
      "x shape Tensor(\"ExpandDims:0\", shape=(1, ?, 8), dtype=float32)\n",
      "step_size Tensor(\"strided_slice:0\", shape=(1,), dtype=int32)\n",
      "Tensor(\"rnn/transpose:0\", shape=(1, ?, 256), dtype=float32)\n",
      "x as output Tensor(\"Reshape:0\", shape=(?, 256), dtype=float32)\n",
      "x shape 256\n",
      "size 1\n",
      "x shape 256\n",
      "size 10\n",
      "logits Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n",
      "sample Tensor(\"strided_slice_3:0\", shape=(10,), dtype=float32)\n",
      "self.probs Tensor(\"strided_slice_4:0\", shape=(10,), dtype=float32)\n",
      "WARNING:tensorflow:From C:\\Users\\adity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "=====\n",
      "BATCH 0\n",
      "====\n",
      "number of times reward 140\n",
      "loss [None, 2.1798012, array([[ -2.35246614e-01,  -2.35674791e-02,   2.49333575e-01,\n",
      "         -7.00309455e-01,  -4.68993247e-01,  -1.22918971e-01,\n",
      "          2.02588990e-01,   3.78602207e-01,   1.22639745e-01,\n",
      "          2.67338604e-01],\n",
      "       [ -2.59991080e-01,   9.79101062e-02,   5.88574409e-01,\n",
      "         -1.17616677e+00,  -1.00992298e+00,  -1.80622905e-01,\n",
      "          2.81932950e-01,   4.16064650e-01,   6.79319575e-02,\n",
      "          5.80288887e-01],\n",
      "       [ -1.22215427e-01,   1.94025844e-01,   6.69101477e-01,\n",
      "         -1.05932367e+00,  -1.11500955e+00,  -2.01632351e-01,\n",
      "          2.30344445e-01,   2.30984211e-01,  -5.98214380e-02,\n",
      "          5.77295840e-01],\n",
      "       [  8.02965537e-02,   2.22386986e-01,   4.81162161e-01,\n",
      "         -4.99866128e-01,  -7.90818632e-01,  -1.56000823e-01,\n",
      "          9.31727812e-02,  -5.35005666e-02,  -1.79902181e-01,\n",
      "          3.12499464e-01],\n",
      "       [  1.82984740e-01,   2.00642496e-01,   2.34710529e-01,\n",
      "         -5.02221286e-04,  -3.87235105e-01,  -1.11317366e-01,\n",
      "         -9.18407179e-03,  -2.22643480e-01,  -2.34781861e-01,\n",
      "          1.24225579e-03],\n",
      "       [  2.36610711e-01,   1.33052528e-01,   6.38544187e-03,\n",
      "          3.76096606e-01,  -2.83160508e-02,  -6.05081096e-02,\n",
      "         -7.51663148e-02,  -3.07534367e-01,  -2.35244945e-01,\n",
      "         -2.38964558e-01]], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  1.,  0.,  2.,  2.])]\n",
      "acts [9, 6, 3, 7, 3, 1]\n",
      "rews [0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001]\n",
      "=====\n",
      "BATCH 1\n",
      "====\n",
      "number of times reward 509\n",
      "loss [None, 0.77766204, array([[-0.74683768, -1.56774163, -1.15315223, -2.93191242, -1.65745378,\n",
      "        -0.66988009,  1.727687  ,  0.57821625,  1.33263433,  2.74343014],\n",
      "       [-1.20702076, -0.92142987,  5.68088007, -2.50414562, -2.45576191,\n",
      "        -0.92488921, -0.03092033,  1.08363914, -0.71187222, -0.70019549],\n",
      "       [ 1.59015083, -0.62003767, -0.72983807, -3.41085982, -2.08233571,\n",
      "        -1.00903523,  1.74557507,  1.24745977, -0.26848641,  1.24531412],\n",
      "       [ 3.84871435,  0.99970347, -2.82766891, -2.11984134, -0.69276196,\n",
      "        -0.66023916,  1.40655029,  0.87321699, -1.36300135, -0.58672428],\n",
      "       [ 5.15251255,  2.27998304, -3.19472051, -0.55432063,  0.38700277,\n",
      "        -0.21264786,  0.60070932,  0.45011276, -2.41928458, -2.66644788],\n",
      "       [ 4.88212919,  2.22233987, -2.90169978, -0.2155697 ,  0.47873038,\n",
      "        -0.1021096 ,  0.36515632,  0.21300629, -2.37506509, -2.91641259]], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  2.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  2.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  2.,  1.,  0.,  0.,  0.,  2.,  2.])]\n",
      "acts [9, 2, 6, 0, 1, 0]\n",
      "rews [0.33333333333333331, 0.33333333333333331, 1.0, 1.0, 1.0, 1.0]\n",
      "=====\n",
      "BATCH 2\n",
      "====\n",
      "number of times reward 853\n",
      "loss [None, 1.49558, array([[ -1.42408049e+00,  -1.80723512e+00,  -1.46660519e+00,\n",
      "         -2.61510110e+00,  -1.59132278e+00,  -5.13877451e-01,\n",
      "          1.61866558e+00,   6.10100091e-01,   2.64060235e+00,\n",
      "          1.82196379e+00],\n",
      "       [ -9.26635563e-01,  -6.17310941e-01,   6.06457567e+00,\n",
      "         -2.39457321e+00,  -1.23573685e+00,  -3.50780308e-01,\n",
      "         -7.03158498e-01,   5.78512430e-01,  -9.72051919e-01,\n",
      "         -8.47130656e-01],\n",
      "       [ -2.18489408e-01,  -1.52882612e+00,   1.28381610e-01,\n",
      "         -3.20877552e+00,  -2.56103015e+00,   6.25223398e-01,\n",
      "          8.65377307e-01,   1.40569711e+00,   7.53423512e-01,\n",
      "          1.16423476e+00],\n",
      "       [  1.19878046e-01,  -5.11123836e-01,   1.83896884e-01,\n",
      "         -1.42792904e+00,  -1.16248822e+00,   4.87084061e-01,\n",
      "          2.04762787e-01,   6.05990469e-01,  -7.72021711e-04,\n",
      "          4.25580472e-01],\n",
      "       [  2.61820972e-01,  -4.95355964e-01,  -8.25667754e-02,\n",
      "         -1.41908777e+00,  -1.23466909e+00,   6.03614211e-01,\n",
      "          1.36816591e-01,   6.13528848e-01,   4.09778431e-02,\n",
      "          4.85891551e-01],\n",
      "       [  2.38287330e-01,  -5.42766333e-01,  -2.83647984e-01,\n",
      "         -1.47202790e+00,  -1.24973166e+00,   5.49531043e-01,\n",
      "          2.14594349e-01,   6.12104058e-01,   2.16732889e-01,\n",
      "          5.93604207e-01]], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 1.,  2.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  2.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  2.,  0.,  1.,  0.,  0.,  2.,  2.])]\n",
      "acts [8, 2, 5, 6, 6, 2]\n",
      "rews [0.33333333333333331, 0.33333333333333331, 0.5, 1.0, 1.0, 1.0]\n",
      "=====\n",
      "BATCH 3\n",
      "====\n",
      "number of times reward 908\n",
      "loss [None, 0.95442396, array([[-1.79827571, -1.94231367, -1.8128581 , -2.06049514, -1.52413666,\n",
      "        -1.14035141,  2.63994956,  1.13622248,  1.90239787,  1.318138  ],\n",
      "       [-0.51775497, -1.61767697,  6.79787064, -1.40333045, -1.73863232,\n",
      "        -0.78098434, -0.12599075, -0.093775  , -0.75978535, -1.0790143 ],\n",
      "       [-0.97883439, -2.23085594, -1.07066393, -3.19018817, -2.63315058,\n",
      "        -0.58448845,  3.83563018,  1.49140191,  0.55877864,  1.3191787 ],\n",
      "       [ 0.51107395,  0.39059746,  0.05915236, -0.49046302, -0.58281851,\n",
      "         0.1463576 ,  0.16010332,  0.03844586, -0.61881608,  0.2402415 ],\n",
      "       [ 1.45471513,  1.53574884, -0.02667934,  0.42670882,  0.1553854 ,\n",
      "         0.79632801, -0.99649256, -0.45422778, -1.41095662, -0.12726167],\n",
      "       [ 1.78105295,  2.03260088, -0.12709936,  0.7902534 ,  0.56194627,\n",
      "         1.08811569, -1.60504556, -0.69420195, -1.63937414, -0.30923814]], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  2.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 2.,  2.,  1.,  0.,  0.,  0.,  2.,  2.])]\n",
      "acts [6, 2, 6, 1, 1, 3]\n",
      "rews [0.33333333333333331, 0.33333333333333331, 1.0, 1.0, 1.0, 1.0]\n",
      "=====\n",
      "BATCH 4\n",
      "====\n",
      "number of times reward 926\n",
      "loss [None, 2.2022836, array([[-1.54844451, -1.53130174, -1.78707254, -2.20271826, -2.0454843 ,\n",
      "        -1.03022468,  1.28381574,  2.31051803,  1.52247488,  1.76915061],\n",
      "       [-0.21277456, -1.29843318,  3.45537043, -1.48946261, -1.33071566,\n",
      "         0.13741443, -0.22614037, -0.2096138 , -0.33550435,  0.02871956],\n",
      "       [-0.20886435, -1.72233558, -0.79049361, -2.56805825, -2.69569707,\n",
      "         0.43245158,  0.87293988,  1.74168575,  0.31024039,  2.02211666],\n",
      "       [ 0.34873331, -0.13410914, -0.2312441 , -0.68689448, -0.89471841,\n",
      "         0.31394753, -0.02448843,  0.39188498, -0.26748902,  0.6977194 ],\n",
      "       [ 0.62257171,  0.2512424 , -0.50317842, -0.19090995, -0.49809438,\n",
      "         0.54872423, -0.25826785,  0.07932407, -0.49863613,  0.47348577],\n",
      "       [ 0.67997772,  0.47491536, -0.67512536,  0.09401247, -0.24900779,\n",
      "         0.61341208, -0.39608622, -0.0922725 , -0.53915894,  0.32928813]], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  0.,  0.,  1.,  2.,  2.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  0.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  0.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  0.,  0.,  1.,  0.,  0.,  2.,  2.])]\n",
      "acts [4, 2, 6, 0, 5, 6]\n",
      "rews [0.20000000000000001, 0.20000000000000001, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331]\n",
      "=====\n",
      "BATCH 5\n",
      "====\n",
      "number of times reward 873\n",
      "loss [None, 2.0515137, array([[ -1.59207225e+00,  -1.52359545e+00,  -1.54675448e+00,\n",
      "         -2.10914540e+00,  -1.89684367e+00,  -8.42282772e-01,\n",
      "          1.50208020e+00,   2.38877249e+00,   1.32350266e+00,\n",
      "          1.17243922e+00],\n",
      "       [ -1.16473651e+00,  -8.25889230e-01,   3.08079171e+00,\n",
      "         -1.19202304e+00,  -1.37969303e+00,   1.29762799e-01,\n",
      "          1.51317239e-01,  -1.90087378e-01,  -2.37086847e-01,\n",
      "          9.46285129e-02],\n",
      "       [ -1.19422460e+00,  -1.62349296e+00,  -2.33220696e-01,\n",
      "         -2.55014825e+00,  -2.82557940e+00,  -2.51891941e-01,\n",
      "          1.28313577e+00,   2.04846311e+00,   7.74400532e-01,\n",
      "          1.66068256e+00],\n",
      "       [ -8.69414508e-02,  -2.82787718e-02,   2.69603491e-01,\n",
      "         -4.12518322e-01,  -6.41146600e-01,  -4.65484485e-02,\n",
      "          4.00256850e-02,   3.27409595e-01,  -1.22937784e-02,\n",
      "          2.80261159e-01],\n",
      "       [ -9.43507701e-02,   2.98514776e-03,  -1.05118632e-01,\n",
      "         -3.83815378e-01,  -7.14160681e-01,  -6.64315373e-03,\n",
      "          5.57207763e-02,   5.03256559e-01,   5.70197888e-02,\n",
      "          3.41508090e-01],\n",
      "       [  2.63357669e-01,   4.00764406e-01,  -4.14007436e-03,\n",
      "          1.86909676e-01,  -1.97745577e-01,   1.40672728e-01,\n",
      "         -3.12469870e-01,   2.31574103e-02,  -1.94700316e-01,\n",
      "          1.62150189e-02]], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  0.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  0.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  0.,  0.,  0.,  1.,  0.,  2.,  2.])]\n",
      "acts [0, 2, 7, 5, 3, 7]\n",
      "rews [0.20000000000000001, 0.20000000000000001, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331]\n",
      "=====\n",
      "BATCH 6\n",
      "====\n",
      "number of times reward 835\n",
      "loss [None, 1.7366681, array([[-1.87211001, -1.69330895, -1.67747593, -2.37279129, -2.28011608,\n",
      "        -0.94950247,  1.86847186,  2.33092451,  1.55101848,  1.20655632],\n",
      "       [-0.4843846 , -1.13923812,  5.38686371, -2.49011755, -1.30132008,\n",
      "         0.4196018 , -0.4335618 , -1.30413437,  0.18998645, -0.74110174],\n",
      "       [-1.08406484, -1.86105239,  0.18491897, -2.84211636, -3.04379702,\n",
      "        -0.44338721,  1.86548758,  1.96383035,  0.84610832,  0.9932189 ],\n",
      "       [-0.13576078,  0.01717719,  0.31270558, -0.4378728 , -0.90205145,\n",
      "        -0.09465349,  0.37920403,  0.3901346 ,  0.01126129,  0.08013725],\n",
      "       [ 0.12760419,  0.48969606,  0.08916714,  0.45235196, -0.35976201,\n",
      "         0.06974654,  0.02884167,  0.01861004, -0.29902583, -0.15995124],\n",
      "       [ 0.24341187,  0.75433993, -0.09275574,  0.96521342, -0.03901689,\n",
      "         0.1539555 , -0.16625939, -0.18209806, -0.45209676, -0.28503454]], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  2.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  2.,  0.,  1.,  0.,  0.,  2.,  2.])]\n",
      "acts [7, 2, 2, 9, 8, 7]\n",
      "rews [0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 1.0, 1.0, 1.0]\n",
      "=====\n",
      "BATCH 7\n",
      "====\n",
      "number of times reward 921\n",
      "loss [None, 1.3089933, array([[ -2.12277913e+00,  -2.15360332e+00,  -1.97116590e+00,\n",
      "         -2.50337601e+00,  -2.72857976e+00,  -1.16518450e+00,\n",
      "          1.77371120e+00,   1.70004809e+00,   2.31514311e+00,\n",
      "          1.98163450e+00],\n",
      "       [  3.96011114e-01,  -6.27609730e-01,   5.73709393e+00,\n",
      "         -1.70616996e+00,  -1.05158973e+00,   2.80108243e-01,\n",
      "         -1.18411529e+00,  -1.09385538e+00,  -1.85140550e-01,\n",
      "         -1.36574101e+00],\n",
      "       [ -7.61843026e-01,  -1.59646225e+00,   1.25568524e-01,\n",
      "         -2.36125016e+00,  -3.26435232e+00,  -3.78197342e-01,\n",
      "          9.78520274e-01,   1.32424831e+00,   1.80318630e+00,\n",
      "          9.68706548e-01],\n",
      "       [ -2.11899728e-03,  -1.34704113e-02,   2.28354931e-01,\n",
      "         -5.05393863e-01,  -1.21027410e+00,  -2.30039448e-01,\n",
      "          2.81756133e-01,   3.21767122e-01,   5.19213080e-01,\n",
      "          7.61921704e-02],\n",
      "       [  3.08271162e-02,   1.59456104e-01,   2.59333383e-02,\n",
      "         -1.58723027e-01,  -9.60661173e-01,  -1.29209906e-01,\n",
      "          2.18041569e-01,   2.28720248e-01,   3.52714211e-01,\n",
      "         -1.31025780e-02],\n",
      "       [ -7.20679015e-03,   2.06934810e-01,  -1.04557648e-01,\n",
      "         -3.09332833e-02,  -8.66195560e-01,  -1.12342045e-01,\n",
      "          2.32544452e-01,   2.00830340e-01,   3.11748952e-01,\n",
      "         -2.30851863e-02]], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  2.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  2.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  2.,  0.,  1.,  0.,  0.,  2.,  2.])]\n",
      "acts [7, 2, 8, 7, 8, 6]\n",
      "rews [0.33333333333333331, 0.33333333333333331, 1.0, 1.0, 1.0, 1.0]\n",
      "=====\n",
      "BATCH 8\n",
      "====\n",
      "number of times reward 895\n",
      "loss [None, 1.4244398, array([[-1.98124433, -2.24673343, -2.15053296, -2.91123486, -2.73965812,\n",
      "        -1.4364413 ,  1.90829718,  1.74894619,  2.04538417,  2.06779552],\n",
      "       [-0.8635323 , -1.43189323,  4.31191635, -1.61082888, -2.03186536,\n",
      "         0.4209404 , -0.42550173, -0.67066091,  0.78078038, -1.07108641],\n",
      "       [-1.29884624, -2.13097978, -0.03893436, -2.73955965, -3.52947712,\n",
      "        -0.94304401,  1.40062201,  1.60243165,  1.57812619,  1.51496625],\n",
      "       [-0.17783704,  0.02793383,  0.22060305, -0.20127344, -0.69167846,\n",
      "        -0.11701483,  0.10553934,  0.18404301,  0.20269567,  0.13898559],\n",
      "       [-0.22964737,  0.14746124,  0.23713128,  0.05511167, -0.69580626,\n",
      "        -0.05668471, -0.00718147,  0.12116356,  0.11684999,  0.11756067],\n",
      "       [-0.22645444,  0.18641236,  0.14731231,  0.13126342, -0.59004128,\n",
      "        -0.04340036, -0.01640119,  0.10006176,  0.08629406,  0.11281009]], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  2.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  2.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  2.,  0.,  1.,  0.,  0.,  2.,  2.])]\n",
      "acts [9, 2, 6, 2, 8, 6]\n",
      "rews [0.33333333333333331, 0.33333333333333331, 1.0, 1.0, 1.0, 1.0]\n",
      "=====\n",
      "BATCH 9\n",
      "====\n",
      "number of times reward 929\n",
      "loss [None, 2.3261511, array([[-1.73475778, -2.63227749, -2.18695927, -2.42844892, -3.03619409,\n",
      "        -1.68055737,  1.8802489 ,  1.42243838,  1.80618989,  2.47231913],\n",
      "       [-0.06029214, -1.27075529,  5.0395298 , -2.49164104, -1.53243363,\n",
      "         0.93501908, -0.84103423, -1.04357469,  0.68559366, -1.91439283],\n",
      "       [-1.82121301, -3.13546443, -0.6555838 , -3.49291062, -3.89241004,\n",
      "        -1.15072811,  2.41608715,  1.61797476,  1.52472758,  1.83350742],\n",
      "       [-0.3687385 , -0.10056908,  1.19429314, -0.71752834, -1.17098212,\n",
      "         0.0490417 ,  0.09704003,  0.06145651,  0.32357556, -0.16623367],\n",
      "       [-0.45057935, -0.29012933,  0.33752406, -0.58059806, -0.87973881,\n",
      "        -0.02164453,  0.29448554,  0.27867988,  0.30541578,  0.17328522],\n",
      "       [-0.46944481, -0.18940791,  0.67959046, -0.46388084, -0.74561292,\n",
      "         0.12979655,  0.05955753,  0.13743125,  0.27995893, -0.04660302]], dtype=float32)]\n",
      "Observation [array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  0.,  1.,  0.,  0.,  2.,  2.])]\n",
      "acts [9, 1, 6, 2, 0, 3]\n",
      "rews [0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331, 0.33333333333333331]\n",
      "([array([ 0.,  0.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  1.,  0.,  0.,  0.,  2.,  2.]), array([ 0.,  2.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  2.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  2.,  0.,  1.,  0.,  0.,  2.,  2.]), array([ 2.,  2.,  0.,  1.,  0.,  0.,  2.,  2.])], [9, 2, 6, 2, 2, 2], [0.33333333333333331, 0.33333333333333331, 1.0, 1.0, 1.0, 1.0])\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
