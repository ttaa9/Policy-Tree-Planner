{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.autograd as autograd\n",
    "import torch.nn as nn, torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable as avar\n",
    "    \n",
    "from SimpleTask import SimpleGridTask\n",
    "from TransportTask import TransportTask\n",
    "from NavTask import NavigationTask\n",
    "from SeqData import SeqData\n",
    "from LSTMFM2 import LSTMForwardModel\n",
    "from GreedyValuePredictor import GreedyValuePredictor\n",
    "\n",
    "import os, sys, pickle, numpy as np, numpy.random as npr, random as r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gumbel(shape, eps=1e-20):\n",
    "    U = torch.rand(shape) #.cuda()\n",
    "    return -avar(torch.log(-torch.log(U + eps) + eps))\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    y = logits + sample_gumbel(logits.size())\n",
    "    return F.softmax(y / temperature, dim=-1)\n",
    "\n",
    "def gumbel_softmax(logits, temperature):\n",
    "    \"\"\"\n",
    "    input: [*, n_class]\n",
    "    return: [*, n_class] an one-hot vector\n",
    "    \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    shape = y.size()\n",
    "    _, ind = y.max(dim=-1)\n",
    "    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "    y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "    y_hard = y_hard.view(*shape)\n",
    "    return y #+ (y_hard - y).detach() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_valueF(state):\n",
    "    state = state.squeeze()\n",
    "    #ForwardModel.printState(state)\n",
    "    vx = torch.sum((state[0:15]-state[34:49]).pow(2))\n",
    "    #print('vx',vx)\n",
    "    vy = torch.sum((state[15:30]-state[49:64]).pow(2))\n",
    "    #print('vy',vy)\n",
    "    value = -( vx + vy ) \n",
    "    return value\n",
    "\n",
    "def greedy_cont_valueF(state):\n",
    "    state = state.squeeze()\n",
    "    _,ix = state[0:15].max(0)\n",
    "    _,gx = state[34:49].max(0)\n",
    "    _,iy = state[15:30].max(0)\n",
    "    _,gy = state[49:64].max(0)\n",
    "    vx = torch.sum((ix - gx)*(ix - gx))\n",
    "    vy = torch.sum((iy - gy)*(iy - gy))\n",
    "    value = -( vx + vy ) \n",
    "    return value\n",
    "\n",
    "def greedy_CE(state):\n",
    "    state = state.squeeze()\n",
    "    _, gx = state[34:49].max(0)\n",
    "    _, gy = state[49:64].max(0)\n",
    "    \n",
    "    px = state[0:15]\n",
    "    py = state[15:30]\n",
    "    \n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    vx = loss(px.unsqueeze(dim=0), gx)\n",
    "    vy  = loss(py.unsqueeze(dim=0), gy)\n",
    "    return - (vx + vy)\n",
    "    \n",
    "def greedy_value_predictor(state):\n",
    "    #state = state.squeeze()\n",
    "    value = GreedyVP(state)\n",
    "    return value.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_model_name = 'LSTM_FM_1_99' \n",
    "gvp_model_name = \"greedy_value_predictor\"\n",
    "# s = 'navigation' # 'transport'\n",
    "# trainf, validf = s + \"-data-train-small.pickle\", s + \"-data-test-small.pickle\"\n",
    "# print('Reading Data')\n",
    "# train, valid = SeqData(trainf), SeqData(validf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTask(px,py,orien,gx,gy):\n",
    "    direction = NavigationTask.oriens[orien]\n",
    "    gs = np.array([gx, gy])\n",
    "    env = NavigationTask(agent_start_pos=[np.array([px,py]), direction],goal_pos=gs)\n",
    "    return env\n",
    "\n",
    "class SimulationPolicy(nn.Module):\n",
    "    def __init__(self,  env, layerSizes=[100,100]):\n",
    "        super(SimulationPolicy, self).__init__()\n",
    "        self.actionSize = len(env.actions)\n",
    "        self.stateSize = len(env.getStateRep(oneHotOutput=True))\n",
    "        self.env = env\n",
    "        print(\"State Size: \" , self.stateSize)\n",
    "        print(\"Action Size: \", self.actionSize)\n",
    "        \n",
    "        # Input space: [Batch, observations], output:[Batch, action_space]\n",
    "        self.layer1 = nn.Linear(self.stateSize, layerSizes[0])\n",
    "        self.layer2 = nn.Linear(layerSizes[0], layerSizes[1])\n",
    "        self.layer3 = nn.Linear(layerSizes[1], self.actionSize)\n",
    "        \n",
    "    def sample(self,state,temperature=2):\n",
    "        output = F.relu( self.layer1(state) )\n",
    "        output = F.relu( self.layer2(output) ) # F.sigmoid\n",
    "        output = self.layer3(output)\n",
    "        #print(output.shape)\n",
    "        soft_output = F.softmax(output, dim=1)\n",
    "        m = nn.LogSoftmax(dim=1)\n",
    "        output = m(output)\n",
    "        return gumbel_softmax(output, temperature), soft_output\n",
    "    \n",
    "    def forward(self, state):\n",
    "        output = F.relu( self.layer1(state) )\n",
    "        output = F.relu( self.layer2(output) ) # F.sigmoid\n",
    "        output = self.layer3(output) \n",
    "        output = F.softmax(output,dim=1)\n",
    "        return output\n",
    "    \n",
    "    def trainSad(self, forwardModel, GreedyVP):\n",
    "        optimizer = optim.Adam(self.parameters(), lr = 0.0005 )\n",
    "        maxDepth = 7\n",
    "        treeBreadth = 2\n",
    "        for p in forwardModel.parameters(): p.requires_grad = False\n",
    "        for p in GreedyVP.parameters(): p.requires_grad = False\n",
    "        cenv = generateTask(0,0,0,14,14)\n",
    "        s0 = avar(torch.FloatTensor([self.env.getStateRep()]), requires_grad=False)\n",
    "        for i in range(0,3000):\n",
    "            tree = Tree(s0,forwardModel,self,greedy_value_predictor, self.env,maxDepth,treeBreadth)\n",
    "            loss = tree.getLossFromLeaves()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if i % 200 == 0: print('Loss',i,\":\",loss.data[0])\n",
    "        \n",
    "# POSSIBLE IDEA\n",
    "# Dont just consider the leaves; consider all the nodes as possible leaves (consider all subpaths too)\n",
    "\n",
    "class Node(object):\n",
    "    \n",
    "    def __init__(self, parent_node, state, action, hidden):\n",
    "        self.parent = parent_node\n",
    "        self.children = []\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.hidden = hidden\n",
    "        \n",
    "    def addChild(self, child):\n",
    "        self.children.append(child)\n",
    "        \n",
    "class Tree(object):\n",
    "    \n",
    "    def __init__(self, initialState, forwardModel, simPolicy, valueF, env,maxDepth=5, branchingFactor=3):\n",
    "        self.simPolicy = simPolicy\n",
    "        self.maxDepth, self.branchFactor = maxDepth, branchingFactor\n",
    "        self.forwardModel = forwardModel\n",
    "        self.valueF = valueF\n",
    "        self.allStates = [initialState]\n",
    "        self.allActions = []\n",
    "        self.env = env\n",
    "        self.forwardModel.reInitialize(1)\n",
    "        parent = Node(None,initialState,None, self.forwardModel.hidden)\n",
    "        self.tree_head = self.grow(parent,0,self.branchFactor)\n",
    "        q, self.leaves = [ parent ], []\n",
    "        while len(q) >= 1:\n",
    "            currNode = q.pop()\n",
    "            for child in currNode.children:\n",
    "                if len( child.children ) == 0: self.leaves.append( child )\n",
    "                else: q.append( child )\n",
    "    \n",
    "    def getPathFromLeaf(self,leafNumber):\n",
    "        leaf = self.leaves[leafNumber]\n",
    "        path = [leaf.state]\n",
    "        actions = [leaf.action]\n",
    "        currNode = leaf\n",
    "        while not currNode.parent is None:\n",
    "            #print(currNode.state)\n",
    "            path.append(currNode.parent.state)\n",
    "            if not currNode.parent.action is None:\n",
    "                actions.append(currNode.parent.action)\n",
    "            currNode = currNode.parent\n",
    "        return (list(reversed(path)),list(reversed(actions)))\n",
    "    \n",
    "    def grow(self,node,d,b,verbose=False):\n",
    "        if verbose: print('Grow depth: ',d)\n",
    "        if verbose: self.env.printState(node.state[0].data.numpy())\n",
    "        if d == self.maxDepth : return node\n",
    "        for i in range(b):\n",
    "            # Sample the current action\n",
    "            hard_action, soft_a_s = self.simPolicy.sample(node.state)\n",
    "            a_s =  [torch.squeeze(hard_action)]\n",
    "            inital_state =  torch.squeeze(node.state)\n",
    "            self.forwardModel.setHiddenState(node.hidden)\n",
    "            current_state, _, current_hidden = self.forwardModel.forward(inital_state,a_s, 1)\n",
    "            current_state = current_state.unsqueeze(dim=0)\n",
    "            self.allStates.append(current_state)\n",
    "            self.allActions.append(a_s)\n",
    "            if verbose: print(\"int_state at depth\",d)\n",
    "            if verbose: self.env.printState(node.state[0].data.numpy())\n",
    "            if verbose: print(\"a_s at depth \",d,\" and breath\",i)\n",
    "            if verbose: print(\"curr_state at depth\",d)\n",
    "            if verbose: self.env.printState(current_state[0].data.numpy())\n",
    "            node.addChild( self.grow( Node(node, current_state, [soft_a_s], current_hidden), d+1, b) )\n",
    "        return node\n",
    "    \n",
    "    def getBestPlan(self):\n",
    "        bestInd, bestVal = 0, avar(torch.FloatTensor( [float('inf')])) #float('-inf')\\n\",\n",
    "        for i, node in enumerate(self.leaves):\n",
    "            currVal = self.valueF(leaf.state)\n",
    "            if currVal.data.numpy() < bestVal.data.numpy():\n",
    "                bestInd = i\n",
    "                bestVal = currVal\n",
    "        return self.getPathFromNode( bestInd )\n",
    "    \n",
    "    def getLossFromLeaves(self):\n",
    "        totalLosses = avar(torch.FloatTensor([0.0]))\n",
    "        for leaf in self.leaves:\n",
    "            totalLosses += -self.valueF( leaf.state )\n",
    "        return totalLosses/len(self.leaves)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Size:  64\n"
     ]
    }
   ],
   "source": [
    "exampleEnv = generateTask(0,0,0,14,14)\n",
    "ForwardModel = LSTMForwardModel(74,64)\n",
    "ForwardModel.load_state_dict( torch.load(f_model_name) )\n",
    "GreedyVP = GreedyValuePredictor(exampleEnv)\n",
    "GreedyVP.load_state_dict( torch.load(gvp_model_name) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Size:  64\n",
      "Action Size:  10\n",
      "Loss 0 : -0.0004462794167920947\n",
      "Loss 200 : -0.001018760958686471\n",
      "Loss 400 : -0.003955319989472628\n",
      "Loss 600 : -0.02479625679552555\n"
     ]
    }
   ],
   "source": [
    "SimPolicy = SimulationPolicy(exampleEnv)\n",
    "SimPolicy.trainSad(ForwardModel,GreedyVP )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
